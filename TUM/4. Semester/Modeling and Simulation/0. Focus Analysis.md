## Complement and norm
- Given a domain $D \subseteq R^n$ 
- We define the **complement** like so:
$$D^c = \mathbb R^n \setminus D$$
- We definethe **Euclidian norm** $||\cdot||$ in $\mathbb R^n$ like so: 
$$||(x_1,...,x_n)^T||= \sqrt{x_1^2+...+x_n^2}$$
## Open and Closed Domains
- A point $x$ in the domain $D \subseteq \mathbb R^n$ is an **inner point**, if there is an $\varepsilon$, such that:
$$B_\varepsilon(x_0)= \{x \in \mathbb R^n\big |||x-x_0||< \varepsilon \}\subseteq D$$
- A domain $D$ is called **open**, if all points in $D$ are inner points

- A point $x_0 \in \mathbb R^n$ is called a **boundry point**, if for any $\varepsilon > 0$: 
$$B_\varepsilon (x_0)\cap D\neq \emptyset \; \land \; B_{\varepsilon} (x_0)\cap D^c \neq \emptyset$$
- The **set of all boundry points is** written as $\partial D$ 
- The **closure** of $D$ is $\overline D = D \cup \delta D$ 
- $D$ is closed, if $\partial D \subseteq D$, meaning $\overline D = D$
- A domain is **bounded**, if:
$$\exists K \in \mathbb R: ||x|| <K, \, \forall x \in D$$
- A domain is **compact** if it is closed and bounded
## Convexity
- A domain is **convex**, if $\forall\, \alpha :  0 \le \alpha \le 1$:
$$\forall \, x, y \in D: \alpha x+ (1-\alpha)y \in D$$
![[convexity.png|300]]
## Continuity
- A vector function $f: D \subseteq \mathbb R^n \to \mathbb R^m$ is continuous at $a \in D$, if, for every sequence $\left ( x^{(k)}\right)_{k \in \mathbb N_0}$ in $D$, with $x^{(k)} \to  a$, there's a corresponding sequence $\left(f\left( x^{(k)}\right) \right)_{k \in \mathbb N_0}$ in $\mathbb R^m$, converging to $f(a)$
- It is continuous on $D$, if it is continuous in every point $a \in D$
# Partial Differentiation
- For more on this topic see [[9. Mehrdimensionale Differenzialrechnung |my notes from last semester]]
- Given a scalar field: 
$$f: D \subseteq \mathbb R^n \to \mathbb R, \, x = (x_1,...,x_n)^T \to f(x) = f(x_1,...,x_n)$$
- We compute the **directional derivative** of $f$ at $a \in D$ in a direction $v$ like so:
$$\frac{\partial f}{\partial v}(a) = \partial_v(a) = f_v(a) = \lim_{h \to 0}\frac{f(a+ hv) - f(a)}{h}$$
- We can compute the **definition of partial derivatives** of $f$ in the point $a$ with respect to the variables $x_i$ like so:
$$\frac{\partial f}{\partial x_i} (a) = \partial _i f(a) = \lim_{h \to 0}\frac{f(a + he_i) - f(a)}{h} \; ,  \, i \in \{1,...,n\}$$
-  A function $f$ is **partially differentiable** in $a \in D$, if all partial derivatives $f_{x_i}$ exist 
- We define the **gradient** of $f$ at $a$ as follows:
$$\nabla f(a) = \mathrm{grad}f(a) = \begin{pmatrix}
f_{x_1}(a)\\
\vdots \\
f_{x_n}(a)
\end{pmatrix}
$$
## Steepest Ascent and Steepest Descent
- A scalar field $f: D \subseteq \mathbb R^n \to \mathbb R$ with continuous partial derivatives $f_{x_1},...,f_{x_n}$ has its **steepest ascent** in the direction $\nabla f(a)$ and its **steepest descent** in the direction $-\nabla f(a)$
## Hessian Matrix
- The partial derivatives $f_{x_1},...,f_{x_n}$ of a field $f: D \subseteq \mathbb R^n \to R$ (where $D$ is open) are scalar fields $f_{x_1},...,f_{x_n}: D \subseteq \mathbb R^n \to \mathbb R$ 
- If these fields are partially differentiable as well, we can define second order derivatives:
$$\partial_{x_j}\partial_{x_i}f(x) = \frac{\partial ^2f}{\partial x_i\partial x_j}(x) = \partial_j\partial_i f(x) = f_{x_jx_i}(x)$$
- Given a second order partially differentiable scalar field $f: D \subseteq \mathbb R^n \to  \mathbb R$, we define the **Hessian Matrix** as follows: 
$$H_f(x)= \begin{pmatrix}
f_{x_1x_1}(x) & ... & f_{x_1x_n}(x) \\
\vdots & & \vdots \\
f_{x_nx_1}(x) & ... & f_{x_nx_n}(x)

\end{pmatrix}$$
- The Hessian Matrix is symmetric
## Jacobi-Matrix
- Going beyond scalar fields, we can extend our definitions for **vector fields**:
$$f: D \subseteq \mathbb R^n\to \mathbb R^m, \, x = \begin{pmatrix}
x_1 \\\vdots\\x_n
\end{pmatrix} \mapsto f(x) = \begin{pmatrix}
f_1(x_1,...,x_n)\\
\vdots \\
f_n(x_1,...,x_n)
\end{pmatrix}
$$
- We define the terms **(continuously) (k-times) partially differentiable** analogously to scalar fields
- If a vector field $f: D \subseteq \mathbb R^n \to R^m$ is partially differentiable on $D$, we define the Jacobian matrix as follows:
$$Df(x)=J_f(x) \left(\frac{\partial f_i}{\partial x_i}(x)\right)_{ij} = \begin{pmatrix}
\frac{\partial f_1}{\partial x_1}(x)& ... & \frac{\partial f_1}{\partial x_n}(x) \\
\vdots && \vdots \\
\frac{\partial f_m}{\partial x_1}(x)& ... & \frac{\partial f_m}{\partial x_n}(x) \\
\end{pmatrix} = 
\begin{pmatrix}
\nabla f_1(x)^T \\ 
\vdots \\
\nabla f_m(x)^T
\end{pmatrix}
$$
### Calculation Rules
- For partially differentiable vector fields $f, g:D \subseteq \mathbb R^n \to \mathbb R^m$, we have the following properties:
- **Additivity**: 
$$D(f+g) = Df(x) + Dg(x)$$
- **Homogeneity** $\forall \lambda \in \mathbb R$:
$$D(\lambda f)(x) = \lambda Df(x)$$
- **Product Rule**:
$$D(f^Tg)(x)= f(x)^TDg(x)+ g(x)^TDf(x)$$
- For two vector fields $f: D \subseteq \mathbb R^n \to \mathbb R^m$ and $g : D' \subseteq \mathbb R^l \to \mathbb R^n$ with $g(D') \subseteq D$, the **Composition Rule** applies:
$$D(f \circ g)(x) = Df(g(x))Dg(x)$$
 
## Operators
### LaPlace Operator
- The **LaPlace operator** of a scalar field $f: D \subseteq \mathbb R^n \to \mathbb R$ is defined as the scalar field $\Delta f$:
$$\Delta f= \sum_{i = 1}^{n} \partial^2_i f$$
### Divergence
- The **divergence** of a vector field $v= (v_1,...,v_n)^T : D \subseteq\mathbb R^n \to \mathbb R^n$ ist definiert als das skalare Feld $\mathrm{div}\, v$
$$\mathrm{div}\,v = \nabla \cdot v = \sum_{i = 1}^{n}\partial_i v_i$$
### Rotation
- The **rotation** of a 3D vector field $v = (v_1,v_2,v_3)^T$ ist definiert als das Vektorfeld $\mathrm{rot} \, v$ 
$$\mathrm{rot} v = \begin{pmatrix}
\partial_2v_3 - \partial_3v_2
\partial_3v_1 - \partial_1v_3
\partial_1v_2 - \partial_2v_1
\end{pmatrix}$$
- The following identities hold:
![[rotation identities.png]]
# Coordinate Transformation
- Beyond cartesian coordinates, different systems can be defined:
## Polar Coordinates
![[polar coordinates.png|250]]
$$\phi : \cases{\mathbb R_{> 0}\times [0, 2\pi[ \to \mathbb R^2\setminus \{0\} \\
\begin{pmatrix}
r\\\varphi
\end{pmatrix} \mapsto
\begin{pmatrix}
x\\ y
\end{pmatrix} = \begin{pmatrix}
r \cos \varphi \\\ r \sin \varphi
\end{pmatrix}
}$$
# Roots and Optima
## Multi Dimensional Newton iteration
- Let $D$ be convex and open and $f: D \subseteq \mathbb R^n \to \mathbb R^n$ be a $C^2$ function
- The algorithm for appoximating a root $x^* \in D$ of $f$, with a tolerance of $\varepsilon$ goes as follows:
- Choose a **starting point** $x_0 \in D$ close to $x^*$
- While $||x_{k+1} - x_k|| \ge \varepsilon$ and $||(Df(x_k))^{-1}f(x_{k + 1})|| \le ||(Df(x_k))^{-1}f(x_k)||$ compute: 
$$x_{k + 1}= x_k - \Delta x_k \qquad \text{with} \qquad Df(x_k)\Delta x_k = f(x_k), \, k = 0, 1, 2,...$$
- TODO
# Curves and Surfaces
- A curve is a mapping 
$$\gamma: I \subseteq \mathbb R \to \mathbb R^n, \, \text{with } \, \gamma(t) = \begin{pmatrix}
x_1(t) \\ 
\vdots \\
x_n(t)
\end{pmatrix}$$
# Quadrature
- For a continuous $f$, the following holds: 
$$\int_c^d \int_a^b f(x, y) \; \mathrm dx \;dy = \int_a^b \int_c^d f(x, y) \; \mathrm dy \;dx$$
