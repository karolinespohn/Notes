# Signale
- Signale sind **zeitabhaengige** und **messbare** physikalische Groessen
- Durch Betrachtung der **Signalaenderungen** koennen **Informationen** uebermittelt werden
## Information 

- Der **Informationsgehalt** eines Zeichens $x \in X$, also wie viele Informationen durch $x$ uebertragen werden, haengt von seiner Auftrittswahrscheinlichkeit $p(x)$ ab
$$I(x) = − \log_2 p(x)$$
## Entropie
- Der mittlere Informationsgehalt einer Signalquelle heisst **Entropie**: 
$$H(X) = \sum_{x \in X}p(x)I(x)= -\sum_{x \in X}p(x)\log_2(p(x))$$
- Entropie misst, wie unvorhersehbar eine Signalquelle ist
- Entropie gibt an, wieviele **bit** benoetigt werden um ein Zeichen der Signalquelle zu kodieren
### Bedingte Entropie
- Die bedingte Entropie zweier Zufallsvariablen $X$ und $Y$ gibt Auskunft ueber die verbleibende Unsicherheit von $Y$, wenn $X$ bereits bekannt ist
$$H(Y|X) = \sum_{x \in X}p(x)H(Y|X = x) = −\sum_{x \in X}p(x)\sum_{y \in Y}p(y|x)\log_2p(y|x)$$
- Gilt $H(Y|X) = H(Y)$, so sind $X$ und $Y$ unabhaengig, sonst gilt stets $H(Y|X) < H(Y)$
### Verbundentropie
- Durch die Verbundentropie $H(X, Y)$ wird angegeben, wie viel Information notwendig ist um die Werte beider Variablen gleichzeitig zu beschreiben
- Die **Verbunddichte** zweier Zufallsvariablen $X$ und $Y$ ist definiert wiefolgt:
$$p(x, y) = \Pr[X = x | Y = y] \cdot \Pr[Y = y]$$
- Die **Verbundentropie** $H(X, Y)$ ist definiert wiefolgt
$$H (X, Y) = - \sum_{x \in X} \sum_{y \in Y}p(x, y) \log_2p(x, y)$$
$$H(X, Y) = H(X) + H(Y | X)$$
# Signaldarstellung
## periodische Signale
- Periodische Signale $s(t)$ lassen sich darstellen als gewichtete Uebereinanderlagerung verschiedener $\sin$- und $\cos$-Signale
- Die Reihenentwicklung von $s(t)$ gelingt mithilfe der Fourierreihe:
$$s(t) = \frac{a_0}2 + \sum_{k = 1}^{\infty}(a_k \, \cos(k\omega t) + b_k\, \sin(k\omega t))$$
## nicht-periodische Signale
- Um nicht-periodische Signale darzustellen verwendet man Fourier-Transformationen:
$$S(f) = \frac{1}{\sqrt{2\pi}}\int_{t = -\infty}^{\infty}s(t)e^{-j2\pi ft} \,\mathrm{d}t = \frac{1}{\sqrt{2\pi}}\int_{t=-\infty}^{\infty}s(t)(\cos(2\pi ft)-j\sin(2\pi ft))\mathrm d t$$
# Signalinterpretation
- Natuerliche Signale sind zeit- und wertkontinuierlich und muessen diskretisiert werden
## Abtasten
- Die Diskretisierung im Zeitbereich bezeichnet man als **Abtasten**
- Durch den Dirac-Impuls $\hat s(t)$ wird das Signal $s(t)$ in aequidistanten Abstaenden $T_a$ erfasst:
$$\hat{s}(t) = s(t) \sum_{n = - \infty}^{\infty} \delta[t - nT_a], \space  \text{mit} \space \delta[t - nT_a] = \begin{cases}
1, \space \text{falls} \space t = nT_a \\
0, \space \text{sonst} \\
\end{cases}$$
### Abtasttheorem von Shannon und Nyquist
- Sei $s(t)$ ein Signal, das auf $|f| \le B$ begrenzt ist
- $s(t)$ kann vollstaendig durch **aequidistante** Abtastwerte $\hat s[n]$ beschrieben werden, wenn die Distanz zweier Stuetzstellen $T_A \le \frac{1}{2B}$ 
###### Beispiel: Abtasten
![[Abtastung.png|400]]
## Rekonstruktion
- Durch die Abtastwerte $\hat s[n]$ kann das urspruengliche Signal $s(t)$  durch [[2. Interpolation|Interpolation]] **rekonstruiert** werden
$$s(t)\approx\sum_{n = -\infty}^{\infty}\hat s[n] \cdot \text{sinc} \left(\frac{t-nT_a}{T_a}\right)$$
###### Beispiel: Abtasten und Rekonstruktion
![[Abtastung Stuetzstellen.png|400]]
![[Rekonstruktion.png|400]]

## Quantisierung
- Die Diskretisierung im Wertbereich bezeichnet man als **Quantisierung**
- Quantisierung geschieht durch die Kodierung von Stufen, auf die das Signal abgebildet wird
- Das Signal wird dabei an den zuvor abgetastete Stellen zu der jeweils naehestliegenden Stufe gerundet
### Quantisierungsfehler
- Gegeben sei ein Signal, das Werte zwischen $a$ und $b$ annimmt, $M  = 2^n$ die Anzahl der Signalstufen und $\Delta = \frac{b - a}{M}$ 
- Die Quantisierungsstufen lauten fuer $i \in \{0, 1, ..., b-1\}$ lauten:
$$a + i \cdot \Delta + \frac {\Delta}{2}$$
###### Beispiel: Quantisierung
![[Quantisierung.png|400]]
# Uebertragungskanaele
- Signale werden ueber Kanaele uebertragen
- Dabei kommt es zu **Daempfung**, **Tiefpassfilterung**, **Verzoegerungen** und zu **Rauschen** 
## Kanalkapazitaet
- Die **Kanalkapazitaet** eines Kanals wird durch die **Bandbreite** $B$ (in $Hz$) und durch die **unterscheidbaren Signalstufen** $M$ begrenzt:
$$C_H = 2B \log_2(M) \text{ bit}$$
- Rauschen erschwert die unterscheidung verschiedener Signalstufen und verhindert das hinzufuegen beliebig vieler Stufen
- Der **Signal to Noise Ratio** liefert ein Mass fuer die Staerke des Rauschens
$$\text{SNR} = \frac{\text{Signalleisttung}}{\text{Rauschleistung}} = \frac{P_S}{P_N}$$
- Der $SNR$ wird in $dB$ angegeben und es gilt: 
$$\text{SNR dB } = 10 \log_{10}(\text{SNR})$$
- Die obere Grenze fuer die Kanalkapazitaet lautet also: 
$$C_S = B \log_2\left(1+\frac{P_S}{P_N}\right) \text{ bit}$$
- $C_S$ beruecksichtigt **additives Rauschen**, aber keine Quantisierungsfehler
- $C_H$ beruecksichtigt die **Signalstufen** und damit das **Quantisierungsrauschen**, aber keine Kanaleinfluesse
- Die tatsaechliche Kanalkapazitaet ist also nach oben beschraenkt durch
$$C < \min\{C_H, C_S\}$$
## Nachrichtenuebertragung
- Bei der Nachrichtenuebertragung werden unterschiedliche Kodierungsphasen durchlaufen
![[Phasen Kodierung.png]]
### Quellenkodierung
- Bei der **Quellenkodierung** werden Nachrichten komprimiert, indem Bitsequenzen auf Codewoerter abgebildet werden
- Redundanzen werden dadurch entfernt und die Uebertragung effizienter
### Kanalkodierung
- Um Bitfehler zu erkennen und korrigieren, die bei der Nachrichtenuebertragung erfolgen, werden wird gezielt **Redundanz** hinzugefuegt
- Die Redundanz wird mittels **Pruefsummen** und **Quittungsprotokollen** umgesetzt
#### Ablauf
- Die uebertragenen Daten werden in Bloecke der Laenge $k$ unterteilt und in Kanalwoerter der Laenge $n > k$ kodiert
- Die $n - k$ zusaetzlichen bit werden fuer **Fehlererkennung** und **Rekonstruktion** verwendet
- Die **Coderate** bezeichnet das Verhaeltnis $R = \frac kn$ 
### Leitungskodierung
- Leitungscodes definieren die Abfolge von bestimmten Grundimpulsen, die Bits oder Bitgruppen repraesentieren
- Eine deartige Abfolge wird **Sendeimpuls** genannt
- Je mehr abrupte Signalwechsel stattfinden, desto hoeher muss die Kanalbandbreite sein
## Eigenschaften
- Wichtige Eigenschaften von Leitungscodes sind die Anzahl der Signalstufen, kodierter Bits pro Symbol und die Schrittgeschwindigkeit
- **Taktrueckgewinnung** versucht, den Sendetakt des Senders aud einem empfangenen Signal zu bestimmen
- **Gleichstromfreiheit** bedeutet, dass das gesendete Signal im Durchschnitt 0 ist
- Bitfehler koennen dadurch vermieden werden, denn eine Folge an $1$ern kann sonst den Kondensator laden 
### Modulation
- Wird ein Kanal fuer mehrere Uebertragungen gleichzeitig verwendet, kommt **Modulation** zum Einsatz
- Mittels **Tiefpassfilterung**, also einer Begrenzung des Spektrums, wird das Sendesignal $s_T(t)$ auf ein **Traegersignal** $f_0$ abgebildet
$$s(t) = s_T(t) \cdot \cos(2\pi f_0t)= \left (\sum^{\infty}_{n = 1} d_n \cdot g_T(t-nT)\right )\cos(2\pi f_0 t)$$

###### Beispiel:
![[Modularisierung.png|400]]
# Uebertragungsmedien
- Uebertragung kann **leitgebunden** oder **nicht-leitgebunden** sein
- Wellen koennen **akustisch** oder **elektromagnetisch** sein
## Wellen
### Elektromagnetische Wellen
- Elektromagnetische Wellen haben eine **elektrische Komponente** $(\overrightarrow E)$ und eine **magnetische Komponente** $(\overrightarrow{B})$
- Im Vakuum breiten sich EWs mit Lichtgeschwindigkeit $c_0$ aus, in einem Medium wird die Ausbreitungsgeschwindigkeit gedaempft:
$$c = vc_0 \quad \text{mit} \quad 0 < v < 1$$
- $v$ und die **Wellenlaenge** $\lambda$ aendern sich proportional zu einander, wobei die **Frequenz** $f = \frac c{\lambda}$ konstant bleibt 