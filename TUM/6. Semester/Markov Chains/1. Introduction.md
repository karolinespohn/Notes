# Definitions
- A discrete stochastic process is a sequence $(X_n)_{n = 0}^\infty$ of random variables, where each variable is  defined on the **same probability space**, and all elements take values in the same **state space** $E$ 
- $E$ is assumed to be **finite** or **countably infinite**
# Markov Chains
- A Markov chain is a **stochastic process** where given the present, the past and the future are **independent** 
- A stochastic process $(X_n)$ is a Markov chain, if for every $n$ and for every $x_0,x_1,...,x_n \in E$, where $P(X_0 = x_0, X_1 = x_1, ..., X_n = x_n) \gt 0$, the following holds: 
$$P(X_n = x_n \mid X_0 = x_0,..., X_{n - 1} = x_{n-1}) = P(X_n = x_n \mid X_{n-1} = x_{n-1})$$
## Homogeneity
- A Markov chain $(X_n)$ is called **homogenous**, if the probability of a transition from state $i$ to state $j$ does not change over time
- Given two states $i, j \in E$ and two points in time $n, m \in \mathbb N_+$, the following holds: 
$$P(X_n = i \mid X_{n-1} = j) = P(X_m = i \mid X_{m-1} = j)$$
## Transition Representations
### Transition Matrix
- In transition matrices $M \in E \times E \to [0; 1]$, the element $p_{i,j}$ represents the probability of a transition from state $i$ to state $j$
### Transition Graphs
- Alternatively, (directional) transition graphs can be used, where each vertex represents a state, and an edge between vertices $i$ and $j\in E$ exist, if $P(X_n =i \mid X_{n-1} = j ) >0$
- An edge $i\to j$ is annotated with the probability $P(X_n = j \mid X_{n-1} = j)$
## Initial distribution
- The initial distribution is the distribution of $X_0$, which takes values on the (finite or infinitely countable) set $E$: 
$$\mu: E \to \mathbb R, \quad \forall i \quad \mu(i) \ge 0 \qquad \text{with } \sum_{i \in E} \mu(i) = 1 \text{ and } \mu(i):= p(x_0 = i)$$
