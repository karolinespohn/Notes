# Least Squares
- The goal is finding $\beta_0$ and $\beta_1$ such that for the resulting regression line $\hat y_i = \beta_0 + \beta_1x_i$ the vertical distance $\sum_{i = 1}^{n}(y_i -\hat y_i)^{2}$ is minimized 
- To this intent, the following function can be minimized: 
$$
S(\beta_0, \beta_1) = \sum (y_i-\beta_0-\beta_1x_i)^2
$$
- To find $\beta_0$ and $\beta_1$ respectively, we regard the derivatives of $S$ with regards to $\beta_{1,2}$ 
$$\frac{\partial}{\partial\beta_0}= -2\sum(y_i-\beta_0-\beta_1x_i) = 0$$
$$\frac{\partial}{\partial\beta_1}= -2\sum(y_i-\beta_0-\beta_1x_i)x_i = 0$$
- We arrive at the **normal equations**:
$$\beta_0 = \overline y -\beta_1\overline x$$
$$\beta_1 = \frac{\sum(x_i -\overline x)(y_i - \overline y)}{\sum(x_i-\overline x)^2}$$
- $\overline x$ and $\overline y$ are defined like so:
$$\overline x= \frac{\sum {x_i}}{n}$$
$$\overline y= \frac{\sum {y_i}}{n}$$
![[Screenshot 2024-10-20 at 16.00.48.png|200]]
# Assumptions
- To perform a linear regression, the following 4 assumptions must hold:
- **Independence of $y$s**: The $y$s must be independent of each other
- **Linearity of mean of $y$**: There must be a linear relationship between $x$ and $y$
- **Homogeneity of variance of $y$**: The variance of errors $\varepsilon$ must be constant across all levels of $x$
- **Normal distribution of $y$**: The residuals of $y$ should follow a normal distribution
# Maximum Likelihood