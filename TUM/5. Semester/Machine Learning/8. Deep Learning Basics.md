$\DeclareMathOperator{\x}{\mathbf x} \DeclareMathOperator{\y}{\mathbf y} \DeclareMathOperator{\w}{\mathbf w}\DeclareMathOperator{\b}{\mathbf b}\DeclareMathOperator{\W}{\mathbf W}\DeclareMathOperator{\a}{\mathbf a}\DeclareMathOperator{\dt}{\, \mathrm d t}\DeclareMathOperator{\dx}{\, \mathrm d x}\DeclareMathOperator{\dany}{\, \mathrm{d}}\DeclareMathOperator{\Var}{\text{Var}}\DeclareMathOperator{\V}{\mathbf V}\DeclareMathOperator{\diag}{\text{diag}}\DeclareMathOperator{\D}{\mathbf D}\DeclareMathOperator{\E}{\mathbb E}$
# DL in Binary Classification
## Logistic Regression
- In [[6. Linear Classification|logistic regression]], posterior distributions were modelled like so: 
$$(y\mid \x)\sim \text{Bernoulli}\left(\sigma(\w^T\x)\right)$$
- $w^Tx$ and $\sigma(a)$ are defined as usual:  
$$\w^T\x := w_0 + w_1x_1...w_nx_n$$
$$\sigma(a):=\frac{1}{1+\exp(-a)}$$
## Visualization
- This can be visualized like so:
![[DL Linear Regression one layer.png|300]]
- The nodes represent **scalar inputs** and they are multiplied with the weight on the edges
- Then the weighted sum $a_0 = \sum_{i = 1}^d = w_ix_i$ is computed and an **activation function** is applied to $a_0$: $f(\x, \w) = \sigma(a_0)$
- Non-linear functions can be handled by applying a basis function to each x: $\sigma(\w^T\phi(\x))$
![[DL with basis fun.png|300]]
- To get from the leaves to the second layer we simply apply the basis function
- To get to the root, we follow the same procedure as we did in the linear case
## Finding w and $\phi$
- $w$ can be found by minimizing the Loss Function
- $\phi$ can actually be replaced through learnable weights as well 
![[DL multiple layers.png|200]]
- Previously, only $100$ and $110$ were learned, now all $w_{ijk}$ are learned, with $i =\text{layer}$, $j = \text{input node}$ and $k = \text{output node}$ 
$$f(\x, \W) = 
\sigma_1\left([w_{100} \quad w_{110}]\cdot \sigma_0
\left( \left[ 
\begin{matrix} 
w_{000}& w_{010} & w_{020}\\ w_{001}& w_{011} & w_{021}\\ \end{matrix}
\right] \left[
\begin{matrix} 1\\x_1\\x_2 \end{matrix}
\right] \right) \right)$$
$$\W^* = \arg \min_{\W}\sum_{n = 1}^{N} - (y_n\log f(\x_n, \W) + (1-y_n)\log(1-f(\x_n, \W)))$$
## More complicated functions
- Models can be made more complicated by adding more **hidden layers**
![[DL model.png|400]]
## Universal Approximation Theorem 
- For any function, there exists a **multi-layer perceptron** that predicts the function arbitrarily well
- Finding it is often complex 
# Beyond Binary Classification
- Depending on the task, different **loss functions** and **final layer application functions** are needed
![[DL fun choice table.png]]
# Parameter Learning
- Oftentimes the loss function $E(\W)$ is non-convex, making optimization tricky
- The default approach for finding a local minimum is gradient descent
- The gradient can be computed...:
	- **by hand**: often tricky
	- **numerically**: requires $\mathcal O(|\W|^2)$ operations
$$\frac{\partial E}{\delta w_{ijk}} = \frac{E(w_{ijk} + \varepsilon) - E(w_{ijk})}{\varepsilon} + \mathcal O(\varepsilon)$$
	- **symbolic differentiation**: automates how one would differentiate by hand, may exponentially grow
	- **automatic differentiation**: eg. backpropagation
## Backpropagation
- In the following, this example with the corresponding computational graph will be used
![[backtracking example fun.png|300]]
![[bt computational graph.png|300]]
- In the first step, the function is written as a composite of modules: 
![[backtracking fun split up.png|300]]
- Then, the local derivative of each module is calculated through symbolic derivation
![[backtracking local derivs.png|300]]
- In a **forward pass**, $f$ is computed for a value $x$ by following the computational graph, and the intermediate values for $a, b, c$ and $d$ are cached
- For $X = -4.499$:
![[bt forward graph.png|300]]
- In a **backward pass**, we use the cached values to compute the local derivatives $\frac{\partial d}{\partial c}$, $\frac{\partial c}{\partial b}$, $\frac{\partial b}{\partial a}$,  $\frac{\partial a}{\partial x}$
- We arrive at the global derivative, by multiplying the local derivatives
![[bt backwards pass.png|300]]
### Multiple Inputs
- For functions with multiple inputs, the derivative along each path is computed
![[bt multiple inputs.png|300]]
![[bt multi inputs calc.png|300]]
### Multiple Paths
- Fot functions with multiple paths from $x$ to $c$, the derivatives along each path are summed
![[bt multi paths.png|300]]
![[bt multi paths calc.png|300]]
![[bt multi paths general.png|300]]
![[bt multi paths general calc.png|300]]
## Jacobian & Gradient
- Given a function $f: \mathbb R^n \to \mathbb R^m$, and $\a = f(\x)$ 
- The **Jacobian** is a $m \times n$ matrix of **partial derivatives**
$$\frac{\partial\a}{\partial\x}=\left[
\begin{matrix}
\frac{\partial a_1}{\partial x_1} & \dots & \frac{\partial a_1}{x_n}\\
\vdots & \ddots & \vdots\\
\frac{\partial a_n}{\partial x_1} & \dots & \frac{\partial a_m}{x_n}\\
\end{matrix}
\right] \in \mathbb R^{m \times n}$$
- Given a function $g: \mathbb R^n \to \mathbb R$, and $c = g(\x)$ 
- The **gradient** $\nabla_{\a}c$ is the transpose of the Jacobian $\frac{\partial c}{\partial \a} \in \mathbb R^{1\times m}$
$$\nabla_{\a}c = \left(\frac{\delta c}{\delta \a}\right)^T = \left[\frac{\partial c}{\partial a_1} \quad \dots \quad \frac{\partial c}{\partial a_m}\right]^T$$
### Chain Rule
- The **chain rule** can be represented using matrices like so: 
$$\frac{\partial c}{\partial x_j} = \sum_{i = 1}^{m}\frac{\partial c}{\partial a_i}\frac{\partial a_i}{\partial x_j}$$
![[chain rule matrix form 1.png|500]]
![[chain rule matrix form 2.png|300]]
# Structured Data
- So far, fully-connected feed-forward layers were discussed
- **Fully connected** means that  each neuron is connected to every neuron in the previous and subsequent layers
- **Feed-forward** means that the information only flows in one direction
- However, there are more types of layers such as **Convolution Layers**, **Recurrent Layers** and **Graph Convolution Layers**
- By chosing an appropriate layer, known properties of different types of data can be leveraged to provide **inductive bias** (assumptions that help the model learn better)
## Convolution
- Convolution layers are used for image processing, exploiting the fact that natural images have high local correlation
- **Continuous convolution** is defined like so: 
$$(x*k)(t)= \int_{-\infty}^{\infty}x(\tau)k(t-\tau)\dany \tau$$
- $(x*k)(t)$ is the mathematical notation for convolution, where $x$ represents the input data, $k$ represents the kernel/filter (the weights being applied) and $t$ represents the position at which we are calculating the convolution
- This calculation yields the weighted average of the input signal $x$ using the weights $k$ at each point in time
- **Discrete convolution** is used by **convolutional neural networks** and it is defined like so: 
$$(x*k)(t) = \sum_{\tau = -\infty}^{\infty}x(\tau) k(t-\tau)$$
- CNNs for images are based on 2D concolution: 
$$(x*k)(i, j) = \sum_l\sum_m x(l, m) k (i - l, j -m)$$
- However in reality they use a slightly adjusted, computationally less expensive version: 
$$\hat x(i, j)= \sum_{l = 1}^L\sum_{m = 1}^Mx(i + l, j + m)k(l, m)$$
- An example is shown here:

![[cnn.png|400]]
 - As seen in the above example, the output is smaller than the input, since there is not enough data at the boundaries
- This scheme is called **VALID**; No padding is used and for a kernel with width $K$, the input sized is decreased to 
$$D_{l + 1} = (D_l-K)+ 1$$
- In the scheme **SAME**, padding of the size $P = \lfloor \frac K2 \rfloor$ is added to preserve the input size
- In the scheme **FULL**, the output size is increased by adding $K - 1$ values to each side
- Input can be padded with zeroes, constant values, or by reflecting/repeating the image
![[cnn padding schemes.png|300]]
- A **stride** is the distance between the positions a kernel is applied
- Strides $> 1$ downsample the image: 
$$D_{l + 1} = \left \lfloor \frac{D_l + 2P-K}{S}\right \rfloor + 1$$
![[cnn strides.png|400]]
- **Pooling** also downsizes by calculating sumary statistics in a window
- This creates an invariance to small movements
![[cnn pooling.png|400]]
# Training Deep Neural Networks 
## Double Descent
- Deep neural networks can be trained to have very low training error, even if they have many more parameters than training examples.
![[double descent.png]]
- It is unclear why
## Weight Initialization
- To train a neural network, the weights have to be changed to minimize the training loss
- The initial assignment of the weights in often crucial
- Two things have to be considered; **weight symmetry** and **weight scale**
### Weight symmetry
- Two different neurons with the same bias and incoming and outcoming weights always get the same gradient
- They cannot extract different features and one of the neurons is redundant
- To avoid this, weights can be initialized to **small random values**
### Weight scale
- Neurons with large **fan-in** (many connections incoming) can satuate if small changes are applied to many of its weights
- Neurons with large **fan-out** (many outgoing connections) cause large weight changes during the optimization step and can cause large changes in the weights of the previous layer in the backwards pass
- Weight scales lead to vanishing or exploding gradients (Anton)
### Xavier Glorot
- To perserve signal properties, weight matrices s a zero mean and the following variance: 
$$\Var(\W) = \frac{2}{\text{fan-in} + \text{fan-out}}$$
### Vanishing & Exploding Gradients
- Deep neural networks sometimes suffer from vanishing or exploding gradients
- This can happen when multiplying an input by a matrix $t$ times: 
$$\W=\V\diag(\D)\V^{-1} \implies \W^t = V(\diag(\D))^tV^{-1}$$
- If $\D_{ii} < 1.0$, $\D^t$ will be close to zero and the gradients will be small, leading to slow convergence of the network
- If $\D_{ii} > 1.0$, $\D^t$ will explodes leading to unstable calculations
- Vanishing gradients can also occur when using saturating activations
# Deep Learning Frameworks
## Static vs. Dynamic Computational Graphs
- In the **static variant** or **define-and-run**, computational graphs are defined first and run afterwards with actual data
- In the **dynamic variant** or **define-by-run**, computational graphs are defined by executing the desired operations
- Static graphs are faster but dynamic graphs are more flexible
# Modern Architecture & Tricks
## Batch Normalization
- The goal of batch normalization is accelerating training by stabilizing the distribution of activation values in each layer, leading to reduced internal covatiate shifts
- In a **normalization step**, the means and variances of layer inputs are fixed 
- The idea is **whitening** (pre-processing raw features) independently
- Since this would be too expensive, the statistics for a minibatch $\mathcal B$ are used 
$$\hat x = \frac{x- \E_{\mathcal B}[x]}{\sqrt{\Var_{\mathcal B}[x]+ \epsilon}}$$
- A scale parameter $\gamma$ and an offset $\beta$ are introduced, learned through backpropagation
$$y = \gamma \hat x + \beta$$
- These parameters let the network **undo** normalization if it needs to, preserving representational power
## Skip Connections
- Skip connections are used to avoid vanishing or exploding gradients
### Highway Connections
- Highway connections add layer input to its output, weighted by the gate $T$
$$y = f (\x, \W )T (\x, \W_T) + \x(1 − T (\x, \W_T ))$$
### Residual Connections
- Residual connections are direct paths that bypass one or more layers in a neural network by adding the input of a layer directly to its output.

![[residual connections.png|300]]
