# Introduction
- Given a matrix of observations $X = \{x_1,x_2,...,x_i\}$, $x_i \in \mathbb R^D$ and a set of targets $y= \{y_1, y_2, ..., y_N\}$, $y_i \in \mathbb R$, the goal is finding a mapping $f(\cdot)$, which maps inputs to targets
$$y_i \approx f(x_i)$$
## Example 
- In this example, $X$ is a set of one dimensional vectors representing housing areas, and $y$ is the price
![[linear regression housing.png|300]]
# Linear Model
- The target $y$ is generated by a function $f(x)$ and an error term $\varepsilon_i$ with $\varepsilon_i \sim  \mathcal N(0, \beta^-1)$  
$$y_i = f(x_i) + \varepsilon_i$$
 - In linear models, we choose a linear function for $f(x)$, where $w$ is a vector of weights, with $w_0$ being the intercept with the $y$-axis
$$\begin{align}
f_w(x_i) &= w_0 + w_1x_{i1} + w_2x_{i2} + ... + w_Dx_{iD}\\
&=w_0 + w^Tx_i
\end{align}$$
- The term can be absorbed, by prepending an $1$ to $x$, and prepending $w_0$ to $w$:
$$\tilde x = (1, x_1, ..., x_D)^T \qquad \qquad \qquad \tilde w= (w_o, w_1,...,w_D)^T$$
- With this simplification, $f_w$ can be written as:
$$f_w(x) = \tilde w^T\tilde x$$
## Loss function
- A **loss function** measures the misfit of our model and the observed data
- The standard choice is **least squares**, which is defined as follows:
$$\begin{align}E_{LS}(w) &= \frac 12 \sum_{i = 1}^N(f_w(x_i) - y_i)^2 \\
& = \frac 12 \sum_{i = 1}^N(w^Tx_i-y_i)^2
\end{align} $$
- The objective is finding a weight vector $w^{\star}$, such that $E_{LS}(w)$ is minimized
$$\begin{align}
w^\star & = \arg \min_w E_{LS}(w) \\
& = \arg \min_w \frac 12 \sum_{i = 1}^N(x_i^Tw-y_i)^2\\
&=\arg \min_w \frac12 (Xw-y)^T(Xw-y)
\end{align}$$
- To minimize the loss $E_{LS}$, we set the gradient $\nabla_w E(w)$ to zero
$$\nabla_wE_{LS} = X^TXw -X^Ty \stackrel{!}{=}0$$
- This leads us to the **normal equation**: 
$$w^\star = X^\dagger y$$
- **Moore-Penrose pseudo-inverse** $X^{\dagger}$ is defined by $$X^\dagger = (X^TX)^{-1}X^T$$ 
- For invertible square matrices, $X^\dagger = X^{-1}$  
# Nonlinear Models
- For a basis function $\phi$ with $\phi_0 = 1$, we define $f$ by 
$$f_w(x) = w_0 + \sum_{i = 1}^N w_i \phi_i(x)$$
- This can be generalized to
$$f(x) = w^T\phi(x)$$
- Although $f$ is not linear in $x$, it is linear in $w$
- $\phi$ can take different forms, depending on type and preferred basis function
![[basis functions.png|300]]
## Finding $w$
- By adjusting the $E_{LS}$ formula slightly, the same method that is used for linear models, can be used here: 
$$E_{LS}(w) = \frac{1}{2} \sum_{i = 1}^N (w^T\phi(x_i) - y_i)^2 = \frac{1}{2} (\Phi w - y)^T(\Phi w - y)$$
- $\phi$ is a matrix, which looks as follows: 
$$\Phi = \begin{pmatrix}
\phi_0(x_1) & \phi_1(x_1) & ... & \phi_N(x_1) \\
\phi_0(x_2) & \phi_1(x_2) & ... & \phi_N(x_2) \\
\vdots & \vdots & ... & \vdots \\
\phi_0(x_N) & \phi_1(x_n) & ... & \phi_N(x_N) \\
\end{pmatrix}$$
- $\phi_i: \mathbb R^D \to \mathbb R$ is a basis function, and $D$ denotes the number of dimensions of an observation
- The optimal weights $w^*$ can be computed as follows: 
$$w^* = (\Phi^T\Phi)^{-1} \Phi^T y = \Phi^{\dagger}y$$
## Choosing degrees
- Finding the correct degree of a regression polynomial is important to avoid underfitting and overfitting
### Train Validation
- The standart train-validation split works for choosing a good degree for a polynomials
### L2 Regularization
- Notably, large weights lead to more overfitting
- The least squares approach can be adjusted as follows to penalize large weights: 
- The larger the regulatization strength $\lambda$ is, the smaller the weights $w$ will be 
![[higher lambda, lower weights.png|300]]

$$E_{\text{ridge}}(w) = \frac{1}{2} \sum_{i = 1}^N (w^T \phi(x_i) - y_i)^2 + \frac{\lambda}{2} ||w||_2^2$$
![[choose poly degree.png|300]]
### Bias-Variance Tradeoff
- The error of an estimator is composed of **bias** and **variance**
![[bias, variance.png|300]]
#### Bias
- Bias is the expected error due to model mismatch
- High bias implies that the model is too rigid, usually because of misspecification or because $\lambda$ is too high
#### Variance
- Variance refers to variation due to randomnes in the training data, oftentimes because it the model memorized the data or the chosen $\lambda$ is too small
- In this case, **overfitting** occurs
#### Minimizing bias and variance
- Minimizing bias and minimizing variance are conflicting goals
- One way of doing this is selecting a model with large capacity and adjusting $\lambda$ to arrive at a good variance
# Probabilistic Formulation
- The aim of regression is finding a function, so that the following term is a good approximation: 
$$y_i = f(x_i) + \varepsilon_i$$
- The **noise** $\varepsilon_i$ is normally distributed $\varepsilon_i\sim\mathcal N(0, \beta^{-1})$, with a fixed precision $\beta = \frac{1}{\sigma^2}$
- The target $y_i$ is also normally distributed with $y_i \sim \mathcal{N}(f(x_i), \beta^{-1})$
## Maximum likelihood
- The distribution of a single sample is given by:
$$p(y_i \mid f_w(x_i), \beta) = \mathcal{N}(y_i \mid f_w(x_i), \beta^{-1})$$
- Assuming the samples are iid, the likelihood of the entire data set is given by: 
$$p(y \mid X, w, \beta) = \prod_{i = 1}^N p(y_i \mid f_w(x_i), \beta)$$
- Maximizing this function is equivalent to minimizing the least quares error function
## Posterior Distribution
- To avoid overfitting due to MLE, one can instead consider the [[4. Probabilistic Inference|posterior distribution]], which is defined like so: 
$$\begin{align}
p(w \mid X, y, \beta, \cdot) &= \frac{p(y \mid X, w, \beta) \cdot p(w \mid \cdot)}{p(y \mid X, \beta, \cdot)}\\\\
& \propto p(y \mid X, w, \beta) \cdot p(w \mid \cdot)
\end{align}$$

![[Posterior Distribution.png|300]]
### Choosing the prior
- The prior $p(w \mid \alpha)$ is set to an isotropic multivariate normal distribution with a mean of zero
- For a distribution with precision $\alpha$ and $M$ elements, it looks like so: 
$$p(w \mid \alpha) = \mathcal{N}(w \mid 0, \alpha^{-1}I) = \left(\frac{\alpha}{2\pi}\right)^{\frac M2}\exp\left(-\frac{\alpha}{2}w^Tw\right )$$
### Maximum A Posteriori
- A MAP estimator can be derived by using the prior distribution and the likelihood
- Maximizing the MAP estimator is equivalent to a ridge regression, which itself is a regression optimized for the $L2$ Norm
### Fully Bayesian Approach
- Since likelihood and prior are Gaussian, and the Gaussian distribution is a conjugate prior to itself, the posterior is also Gaussian, with: 
$$w_{\text{MAP}}  = \mu = \beta \Sigma\Phi^Ty= \alpha I + \beta \Phi^T \Phi $$
## Predicting new data
- The goal of regression is usually to be able to predict the target $\hat y_{new}$ for a datapoint $x_{new}$
- For **maximum likelihood**: 
$$p(\hat \y_{new} \mid \x_{new}, \w_{ML}, \beta_{ML}) = \mathcal N (\hat \y_{new} \mid \w^T_{ML}\phi(\x_{new}) , \beta_{ML}^{-1})$$
- For **maximum a posteriori**: 
$$p(\hat \y_{new} \mid \x_{new}, \w_{MAP}, \beta) = \mathcal N (\hat \y_{new} \mid \w^T_{MAP}\phi(\x_{new}) , \beta^{-1})$$
- Using the **posterior distribution**: 
$$\begin{align}p(\y_\text{new} \mid \x_\text{new}, \mathcal{D}) &= \int p(\y_\text{new} \mid \x_\text{new}, \w) \cdot p(\w \mid \mathcal{D}) \mathrm d \w =\\
&= \mathcal N(\hat{y}_{new}\mid \mu^T\phi(\x_{new}), \beta^{-1} + \phi(x_{new})^T\Sigma\phi(\x_{new}))
\end{align}
$$