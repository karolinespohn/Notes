# Introduction
- Given a matrix of observations $X = \{x_1,x_2,...,x_i\}$, $x_i \in \mathbb R^D$ and a set of targets $y= \{y_1, y_2, ..., y_N\}$, $y_i \in \mathbb R$, the goal is finding a mapping $f(\cdot)$, which maps inputs to targets
$$y_i \approx f(x_i)$$
## Example 
- In this example, $X$ is a set of one dimensional vectors representing housing areas, and $y$ is the price
![[linear regression housing.png|300]]
# Linear Model
- The target $y$ is generated by a function $f(x)$ and an error term $\varepsilon_i$ with $\varepsilon_i \sim  \mathcal N(0, \beta^-1)$  
$$y_i = f(x_i) + \varepsilon_i$$
 - In linear models, we choose a linear function for $f(x)$, where $w$ is a vector of weights, with $w_0$ being the intercept with the $y$-axis
$$\begin{align}
f_w(x_i) &= w_0 + w_1x_{i1} + w_2x_{i2} + ... + w_Dx_{iD}\\
&=w_0 + w^Tx_i
\end{align}$$
- The term can be absorbed, by prepending an $1$ to $x$, and prepending $w_0$ to $w$:
$$\tilde x = (1, x_1, ..., x_D)^T \qquad \qquad \qquad \tilde w= (w_o, w_1,...,w_D)^T$$
- With this simplification, $f_w$ can be written as:
$$f_w(x) = \tilde w^T\tilde x$$
## Loss function
- A **loss function** measures the misfit of our model and the observed data
- The standard choice is **least squares**, which is defined as follows:
$$\begin{align}E_{LS}(w) &= \frac 12 \sum_{i = 1}^N(f_w(x_i) - y_i)^2 \\
& = \frac 12 \sum_{i = 1}^N(w^Tx_i-y_i)^2
\end{align} $$
- The objective is finding a weight vector $w^{\star}$, such that $E_{LS}(w)$ is minimized
$$\begin{align}
w^\star & = \arg \min_w E_{LS}(w) \\
& = \arg \min_w \frac 12 \sum_{i = 1}^N(x_i^Tw-y_i)^2\\
&=\arg \min_w \frac12 (Xw-y)^T(Xw-y)
\end{align}$$
- To minimize the loss $E_{LS}$, we set the gradient $\nabla_w E(w)$ to zero
$$\nabla_wE_{LS} = X^TXw -X^Ty \stackrel{!}{=}0$$
- This leads us to the **normal equation**: 
$$w^\star = X^\dagger y$$
- **Moore-Penrose pseudo-inverse** $X^{\dagger}$ is defined by $$X^\dagger = (X^TX)^{-1}X^T$$ 
- For invertible square matrices, $X^\dagger = X^{-1}$  
# Nonlinear Models
- For a basis function $\phi$ with $\phi_0 = 1$, we define $f$ by 
$$f_w(x) = w_0 + \sum_{i = 1}^N w_i \phi_i(x)$$
- This can be generalized to
$$f(x) = w^T\phi(x)$$
- Although $f$ is not linear in $x$, it is linear in $w$
- $\phi$ can take different forms, depending on type and preferred basis function
![[basis functions.png|300]]
## Finding $w$
- By adjusting the $E_{LS}$ formula slightly, the same method that is used for linear models, can be used here: 
$$E_{LS}(w) = \frac{1}{2} \sum_{i = 1}^N (w^T\phi(x_i) - y_i)^2 = \frac{1}{2} (\Phi w - y)^T(\Phi w - y)$$
- $\phi$ is a matrix, which looks as follows: 
$$\Phi = \begin{pmatrix}
\phi_0(x_1) & \phi_1(x_1) & ... & \phi_N(x_1) \\
\phi_0(x_2) & \phi_1(x_2) & ... & \phi_N(x_2) \\
\vdots & \vdots & ... & \vdots \\
\phi_0(x_N) & \phi_1(x_n) & ... & \phi_N(x_N) \\
\end{pmatrix}$$
- $\phi_i: \mathbb R^D \to \mathbb R$ is a basis function, and $D$ denotes the number of dimensions of an observation
- The optimal weights $w^*$ can be computed as follows: 
$$w^* = (\Phi^T\Phi)^{-1} \Phi^T y = \Phi^{\dagger}y$$
## Choosing degrees
- Finding the correct degree of a regression polynomial is important to avoid underfitting and overfitting
### Train Validation
- The standart train-validation split works for choosing a good degree for a polynomials
### L2 Regularization // TODO!
- Notably, large weights lead to more overfitting
- The least squares approach can be adjusted as follows to penalize large weights: 
- The larger the regulatization strength $\lambda$ is, the smaller the weights $w$ will be 
![[higher lambda, lower weights.png|300]]

$$E_{\text{ridge}}(w) = \frac{1}{2} \sum_{i = 1}^N (w^T \phi(x_i) - y_i)^2 + \frac{\lambda}{2} ||w||_2^2$$
![[choose poly degree.png|300]]
### Bias-Variance Tradeoff
- The error of an estimator is composed of **bias** and **variance**
![[bias, variance.png|300]]
#### Bias
- Bias is the expected error due to model mismatch
- High bias implies that the model is too rigid, usually because of misspecification or because $\lambda$ is too high
#### Variance
- Variance refers to variation due to randomnes in the training data, oftentimes because it the model memorized the data or the chosen $\lambda$ is too small
- In this case, **overfitting** occurs
#### Minimizing bias and variance
- Minimizing bias and minimizing variance are conflicting goals
- One way of doing this is selecting a model with large capacity and adjusting $\lambda$ to arrive at a good variance