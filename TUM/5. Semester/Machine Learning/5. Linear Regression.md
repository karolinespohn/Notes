- Given a set $X = \{x_1,x_2,...,x_i\}$, $x_i \in \mathbb R^D$ of ovservations and a set $y= \{y_1, y_2, ..., y_N\}$, $y_i \in \mathbb R$, the goal is finding a mapping $f(\cdot)$ from mappings to targets
$$y_i \approx f(x_i)$$
- $y$ is generated by a function $f(x)$ and an error term $\varepsilon_i$ with $\varepsilon_i \sim  \mathcal N(0, \beta^-1)$  
$$y_i = f(x_i) + \varepsilon_i$$
 - We choose a linear function for $f(x)$, where $w$ is a vector of weights, with $w_0$ being the intercept with the $y$-axis
$$\begin{align}
f_w(x_i) &= w_0 + w_1x_{i1} + w_2x_{i2} + ... + w_Dx_{iD}\\
&=w_0 + w^Tx_i
\end{align}$$
- The term can be absorbed, by prepending an $1$ to $x$, and prepending $w_0$ to $w$:
$$\tilde x = (1, x_1, ..., x_D)^T \qquad \qquad \qquad \tilde w= (w_o, w_1,...,w_D)^T$$
- With this simplification, $f_w$ can be written as:
$$f_w(x) = \tilde w^T\tilde x$$
## Loss function
- A **loss function** measures the misfit of our model and the observed data
- The standard choice is **least squares**, which is defined as follows:
$$\begin{align}E_{LS}(w) &= \frac 12 \sum_{i = 1}^N(f_w(x_i) - y_i)^2 \\
& = \frac 12 \sum_{i = 1}^N(w^Tx_i-y_i)^2
\end{align} $$
- The objective is finding a weight vector $w^{\star}$, such that $E_{LS}(w)$ is minimized
$$\begin{align}
w^\star & = \arg \min_w E_{LS}(w) \\
& = \arg \min_w \frac 12 \sum_{i = 1}^N(x_i^Tw-y_i)^2\\
&=\arg \min_w \frac12 (Xw-y)^T(Xw-y)
\end{align}$$
- To minimize $E_{LS}$, we set the gradient $\nabla_w E(w)$ to zero
$$\nabla_wE_{LS} = X^TXw -X^Ty \stackrel{!}{=}0$$
- This leads us to the **normal equation**: 
$$w^\star = X^\dagger y$$
- **Moore-Penrose pseudo-inverse** $X^{\dagger}$ is defined by $$X^\dagger = (X^TX)^{-1}X^T$$ 
