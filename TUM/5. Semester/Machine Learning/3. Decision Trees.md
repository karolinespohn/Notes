# Introduction
- The overall concept of a decision tree is clear: 
![[simple dt.png|300]]
- In classification a data set can be split up repeatedly into different classes
- A new sample $x$ can be classified by testing the attributes of $x$ to find, which region $\mathcal R$ the sample belongs to
- The class distribution $\mathcal n_{\mathcal R} = (\mathcal n_{c_1,\mathcal R},\mathcal n_{c_2, \mathcal R},...,\mathcal n_{c_k,\mathcal R})$ for $C = \{c_1,...,c_k\}$ can then be checked 
- The probability that $x \in \mathcal R$ should be classified as belonging to class $c$ is 
$$p(y=c\mid\mathcal R) = \frac{n_{c, \mathcal R}}{\sum_{c_i \in C}n_{c_i, \mathcal R}}$$
![[decision tree.png|500]]
## Comparison to knn
- As opposed to KNN, when using decision trees one does not need the entire training set during classification, but just the tree resulting from it
- DTs have a much better complexity with regards to memory and inference than knn, as well as a more flexible decision function
## Finding the ideal tree
- To find a tree that performs well, the same process is followed as finding [[2. k-nearest-neighbours|knn]] hyperparameters, but finding the ideal tree is np-complete
-  A decent approach is growing the tree top down, chosing the best split using a **greedy heuristic**
### Suitable Criteria
-  The impurity $i(t)$ at a node $t$ should be at its **maximum** if the classes are equally distributed and at its **minimum** if the classes are pure  
- It should be **symmetric**
- The black line in the right screenshot would be bad, since it would be harder to decrease $i$ the purer it is, promoting more splits than we may want
![[dt functions.png|200]]![[bad dt function.png|200]]
### Impurity Measures
- $\pi_c = p(y = c \mid t)$ describes the probability of $y$ being in class $c$, given we are in node $t$
- Impurity can be defined through the following measures: 
- Missqualification rate:
$$i_E(t) = 1 - \max_c \pi_c$$
- Entropy: ($\lim_{x \to 0+} x\log_2 x = 0$)
$$i_H(t) = -\sum_{c_i \in C} \pi_{c_i}\log_2\pi_{c_i}$$
- Gini index: 
$$i_G(t) = 1 - \sum_{c_i \in C}\pi^2_{c_i}$$
# Overfitting
- Overfitting happens, when a model is trained almost perfectly on the training data, but is therefore **unable to generalize**
- The training error is low, possibly $0$, but the validation error is high
- As seen by this graph, increased training performance does not necessarily mean increased validation performance
![[Overfitting Graph.png|300]]
- The test set should only be used once, although big companies sometimes train on test data to improve how they compare to others
![[Modelling training.png|300]]
## K-Fold Cross Validation
### Problem
- Deciding how much data should be used for training and validation respectively is a tradeoff
- If the training set is too small, the model will be weak, but if the validation set is too small, the model can't be tested in all conditions
### Approach
- In K-Fold Cross Validation, the original training set is split into $k$ random subsets
- Each of the $k$ subsets is used as the test set once, while the others are used as the training set
- Each time the model is fitted and validated on the training and test set respectively, the evaluation score is saved and the model is discarded
- To find a good hyperparameter $h$, the different training and validation sets can be tested by assigning a certain value to $h$ and seeing how the model performs
- The best hyperparameter averaged over the different trained models can then be used to train the "real" model on the whole training
 ![[K Fold Cross Validation.png|400]]
### LOOCV
- $\hat = N-$fold cross validation
- Leave one out cross validation is an extreme version of this, where only one element is left out of the training set and each element is used for validation once
- It is good for validation but in many cases it is very expensice
## Avoiding Overfitting through pre-pruning
- The following are possible criteria to determine when to stop a tree from growing:
- **Pure distribution in a branch**: Not a good criterion since it leads to overfitting
- **Maximum depth reached**: Independent of how much gain could be achieved through further splits, the tree is only allowed to reach a certain depth
- **Samples per branch**: In every node, there need to be at least a certain number of samples 
- **Benefit of Split below threshold**: $\Delta i(s, t) \lt t_{\Delta}$
- **Accuracy on the validation set**
# Decision Trees for regression
- Mean-squared-error can be used as a heuristic for splits
![[DTs for regression.png|400]]
# Ensembles
- In ensembles, predictions of diverse classifiers are used to improve performance
- Reduces the variance of the model
## Bagging (bootstrap aggregating)
- The training set is split, and each classifier is trained with different hyperparameters and the predictions are combined
- To reduce the correlation between bagged decision trees, only a subset of features is used to learn each tree ( = **random forest**)
## Boosting
- Each classifier leaves a certain level of error
- Weaker classifiers are trained that focus on correcting the mistake of the previous classifier
- The classifiers sequentially slightly correct the mistakes of the previous model