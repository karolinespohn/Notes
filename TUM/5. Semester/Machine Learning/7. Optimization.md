$\DeclareMathOperator{\x}{\mathbf x} \DeclareMathOperator{\y}{\mathbf y} \DeclareMathOperator{\w}{\mathbf w}\DeclareMathOperator{\b}{\mathbf b}$
- As seen in [[5. Linear Regression| linear & logistic regression]], optimization is central to machine learning
# Convexity 
## Sets
- A set $X$ is convex $\Leftrightarrow$ for all $x, y \in X$, this follows: 
$$\lambda x + (1-\lambda() y \in X, \quad \text{for } \lambda \in [0, 1] $$
- Here, the left set is convex, whereas the right set is not
![[Convex Set.png|300]]
## Functions
- $f(\x)$ is a convex function on the convex set $X$ $\Leftrightarrow$ for all $\x, \y \in X$, this follows: 
$$\lambda f(\x) + (1-\lambda ) f(\y) > f(\lambda \x + (1-\lambda) \y), \quad \text{for } \lambda \in [0, 1]$$
- The region above a convex function is convex, and there are no local minima that are not also global minima 
![[Convex function.png|300]]
### Utilities
- For convex functions, we can determine an upper bound for the function values: 
$$f((1- t)\x + t\y) \le (1-t )f(\x) +tf(\y)$$
- We can also put a lower bound on the function like so: 
$$f(\y) - f(\x) \ge \frac{f((1-t)\x + t\y) - f(\x)}{t}$$
- For a differentiable function $f: X \to \mathbb R$, where $X$ is convex, $f$ is covex $\Leftrightarrow$ this holds:  
$$f(\y) \ge f(\x) + (\y - \x)^T\nabla f(\x)$$
- This is called **first order convexity**
### Verifying Convexity
-  We can prove that a function is convex using the definition or first order convexity
- However, it is oftentimes easier to show that the function can be obtained from simpler convex functions 
- We start with simple convex functions like $f(x) = \text{const}$, $f(\x) = \x^T\cdot b$ or $f(x) = e^x$ 
- Then, we can apply any of these rules: 
- Let $f_1: \mathbb R^d \to \mathbb R, \, f_2: \mathbb R^d \to \mathbb R$ be convex and $g: \mathbb R^d \to \mathbb R$ be concave
	- $h(x) = f_1(\x) + f_2(x)$ is convex
	- $h(x) = \max\{f_1(\x) , f_2(x)\}$ is convex
	- $h(x) = c \cdot f_1(\x)$ is convex for $c \ge 0$
	- $h(x) = c \cdot g(\x)$ is convex for $c \le 0$
	- $h(x) = f_1(A\x + \b)$ is convex
	- $h(x) = m(f_1(\x))$ is convex if $m: \mathbb R\to \mathbb R$ is  convex and nondecreasing
# Gradient Descent
- Gradient descent is an iterative optimization algorithm that seeks the minimum of a function by moving in the direction opposite to its gradient (steepest descent)
- **Line search** is an improvement of gradient descent, which helps find a better step size
$$t^* = \arg \min_{t > 0} f(\theta + t \cdot \Delta \theta)$$
## Algorithm
- Given a start $\theta$, the following steps are repeated until the end-condition is satisfied:
1. Compute the negative of the gradient $\Delta \theta = - \nabla f(\theta)$
3. Chose the step size $t$
4. Update $\theta$ to be the new position
## Learning Rate
- Instead of chosing $t$ using line search, a **learning rate** $\tau$ can be chosen
- If $\tau$ is chosen to small, convergence is slow and it is possible to get stuck in local minima
- If $\tau$ is too large, oscillations are larger and in some cases the gradient descent doesn't converge
### Types
- $\tau$ can be set, such that it **decreases in later iterations**
- A **momentum** variable can be chosen to accelerate the descent as long as the direction stays the same
- $\tau$ can be different for different parameters and it can be smaller when the gradient is large
# Newton Method
- Refer to [[2. Interpolation|this]]
- The idea of the Newton Method is to interpolate the function in the area around the point $\theta_t$
- For a convex function $f$, this works like so: TODO!
- The Taylor expansion at a point $\theta_t$ is defined like so
$$f(\theta_t + \delta) = f(\theta_t) + \delta^T\nabla f(\theta_t) + \frac 12\delta^T\nabla f(\theta_t) + \frac 12\delta^T\nabla^2f(\theta_t) \delta + \mathcal O(\delta^3)$$
- Minimizing this, we arrive at
$$\theta_{t+1}\leftarrow \theta_t-\left[\nabla^2 f(\theta_t)\right]^{-1}\nabla f(\theta_t)$$
- This step is repeated until convergence
## Advantages & Disadvantages
- Newton has a good **rate of convergence** and requires **few passes through data**, 
- The aggregation of Gradient and Hessian can be parallelized, and the gradient requires only $\mathcal O(d)$ data, with $d = \text{number of dimensions}$
- However, the Hessian lies in $\mathcal O(d^2)$ and an update step lies in $\mathcal O(d^3)$, and is not parallizable
# Stochastic Gradient Descent
- Higher-order techniques are too expensive for high-dimensional data
- In stochastic gradient descent, a random subset of the data is chosen and the expectation is calculated with a smaller sample: 

$$\mathbb E_{i \sim \{1,...,n\}}[L_i(\theta)]\approx \frac{1}{|S|}\sum_{j \in S} (L_j(\theta))$$
## Algorithm
1. Select a mini-batch (random, small subset of the data) $S$ 
2. Compute the gradient based on the mini-batch
3. Update $\theta_{t + 1} \leftarrow \theta_t - \tau\cdot\frac{n}{|S|}\cdot \sum_{j \in S}\nabla L_j(\theta_t)$
4. Pick a new mini batch and repeat 2 
- Data is often sampled in a way such that no data point is used twice
- Iterating through all points is called an **epoch**
# Distributed Learning
- In training, the computation is often distributed over different machines
- Communicating the work done by one machine to other machines is a challenge
- In **data parallelism**, multiple replicas of a model work on different subsets of the data at the same time
- In **model parallelism**, many models work on different machines
![[data driven parallelism.png|200]]  ![[model driven parallelism.png|200]]