# 1NN
- Given a data set $\mathcal D = \{(x_i, y_i)\}^n_{i = 1}$ where $x_i$ are the features and $y_i$ are class labels
- A new observation can be classified by defining a distance measure and giving it the same lable as it's nearest neighbour
![[1nn.png|300]]
- However, this results in a poor generalisation, and observation on the borders of 2 generalisation areas cannot be classified
# KNN
- A more robust way of classifying the observation is by looking at the $k$ nearest neighbours, and choosing the (ideally **weighted**) majority label
- KNN is a lazy classifier - we need all the training data during classification
## Formula
- Let $\mathcal N_k(\mathbf x)$ be the $k$ nearest neighbours of a vector $\mathbf x$ and let $\mathrm d(\mathbf x, \mathbf x_i)$ be a distance measure between $\mathbf x$ and $\mathbf x_i$ 
$$p(y = c\mid \mathbf x, k) = \frac 1Z \sum_{i \in \mathcal N_k (\mathbf x)}\frac{1}{\mathrm d(\mathbf{x}, \mathbf{x}_i)}\cdot \mathbb{I}(y_i = c)$$
- The predicted lable $\hat y$ is defined by: 
$$\hat y = \arg \max_{c}(y= c \mid \mathbf x, k)$$
## Chosing $K$ 
- The goal when chosing a $k$ is to find the $k$ that will perform best on unseen future data
- The dataset is split into a training set and a validation set
![[Training, validation, learning, test set.png|400]]
- After the training, a test set is used as a final validator, to avoid overfitting the predictions to the training set
## Scaling Issues
- Since knn relies on distance measures, scaling issues can occur when features of a dataset have different units
![[Scaling Issues.png|400]]
- In this example, the order of one of the features lies in meters and the order of the other feature lies in cm
### Avoiding scaling issues
- Scaling issues can be avoided by **standardizing** each feature to mean zero and unit variance: 
$$x_{i, \text {std}}= \frac{x_i-\mu_i}{\sigma_i}$$
- Alternatively, by using the **Mahalanobis distance**, data can be normalized: 
$$\text{mahalanobis}(x_1, x_2) = \sqrt{(x1 − x2)^T \Sigma^{-1}(x1 − x2)}$$
$$\Sigma = \begin{pmatrix}
\sigma_1^2 & 0 & 0 & \cdots & 0 \\
0 & \sigma_2^2 & 0 & \cdots & 0 \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
0 & 0 & 0 & 0 & \sigma_n^2
\end{pmatrix}$$
## Curse of Dimensionality
- With the same amount of points, less space is covered in higher dimensions: 
- In the following we call each space covered, if it contains a pont
- Let $x \in \{1, 2,..., 10\}$
- With 20 samples, this one dimensional space is $100\%$ covered
![[Curse of Dimensionality 1 Dimension.png|300]]
- However, with the same amount of points, a 2 dimensional space is only covered to $18\%$ and a 3 dimensional space is only covered by $2\%$
![[Curse of dims 2 dims.png|200]]![[Curse of Dims 3 dims.png|200]]
- This poses the problem for knn that the nearest neighbour may not actually be too closely related in many dimensions, since so little space is covered
