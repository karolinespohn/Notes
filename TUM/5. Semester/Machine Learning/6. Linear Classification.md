$\DeclareMathOperator{\x}{ÃŸ\mathbf x} \DeclareMathOperator{\y}{\mathbf y} \DeclareMathOperator{\w}{\mathbf w}$
# Classification 
- In classification, a set of classes $\mathcal C = \{1,...,C\}$ and a set of obeservations $X = \{x_1,...,x_N\}, \, x_i \in \mathbb R^D$ with the lables $y= \{y_1,...,y_N\}, \, y_i \in \mathcal C$ are given
- The goal is finding a function $f: \mathcal R^D \to \mathcal C$, that maps observations $x_i$ to class lables $y_i$
$$y_i = f(x_i), \quad \text{for } i \in \{1,...,N\}$$
## Zero-one loss
- **Zero-one loss** measures the quality of a prediction $\hat y := f(X)$ and it denotes the number of misclassified samples
$$\ell_{01}(y, \hat y) = \sum_{i = 1}^N\mathbb I (\hat y_i = y_i) $$
# Hyperplane Decision Borders
- In 2 class problems, **hyperplanes** can serve as decision boundaries
- Using a vector $w$, which is normal to the hyperplane,  and an offset $w_0$, it is defined like so: 
$$w^Tx+w_0 \begin{cases}
= 0 \quad \text{if $x$ on the plane}\\
> 0 \quad \text{if $x$ on the normal's side}\\
< 0 \quad \text{else}\\

\end{cases}$$
- A data set is called **linearly separable**, if a hyperplane exists such that all $x_i$ with $y_i = 0$ are on one side, and all $x_i$ with $y_i = 1$ are on the other side
## Perceptron Algorithm
- The **perceptron algorithm** is one of the easiest classification algorithms
- We define $f$ like so: 
$$f(t) = \begin{cases}
1 \quad \text{if }t >0\\
0 \quad \text{otherwise}
\end{cases}$$
- Our decision rule is
$$\hat y = f(w^Tx+w_0)$$
### Learning rule
- The parameters $w, w_0$ can be initialized so any value, for instance a $0$ vector
- Whenever a sample in the training set is misclassified, the training set is updated like so:
$$w \leftarrow \begin{cases}
w + x_i \quad \text{if }y_i = 1\\
w - x_i \quad \text{if }y_i = 0
\end{cases}$$
$$w_0 \leftarrow \begin{cases}
w_0 + 1 \quad \text{if }y_i = 1\\
w_0 - 1 \quad \text{if }y_i = 0\\
\end{cases}$$
- If a discriminating between two classes exists, perceptron finds it in a finite number of steps
## Multiple Classes
- To make classification decisions using more than 2 classes, different approaches can be used
### One-vs-Rest Classifier
- Multiple hyperplanes are used, which decide, wheter something is part of a class, or whether it isn't
$$\text{class } \mathcal C_i \leftrightarrow \text{not class } \mathcal C_i$$
- However, this results in ambiguity (the green section), rendering it useless at times
![[One vs Rest.png|300]]
### One-vs-One Classifier
- Instead of deciding wheter something is in a class or outside of it, One-vs-one decides whether something is rather in one certain class, or in another certain class
$$\text{class } \mathcal C_i \leftrightarrow \text{class } \mathcal C_1$$
- The majority vote is used for the classification, but there can be areas of uncertainty
- This approach doesn't scale well
![[One vs One.png|300]]
### Multiclass discriminants
- A hyperplane is defined for each class, representing the decision boundary for that class
- An $x$ is classified in the class, where the decision boundary is furthest away, meaning it is clearest it is in that class
$$f_c = w^T_cx + w_{0c}$$
$$\hat y= \arg \max_{c\in \mathcal C}f_c(x)$$
![[Multiclass Discriminant.png|300]]
## Non-linearly-separable classes
- If 2 classes are non-linearly separable, a nonlinear transformation can be applied 
![[Non linearly separable classes.png|400]]
# Probabilistic Models for Linear Classification
- One classification approach is modelling the distribution of the class label $y$ given the sample $x$
$$p(y = c\mid x) = \frac{p(x\mid y = c)\cdot p(y=c)}{p(x)}$$
- This is called **linear discriminant analysis**
- There are discriminative and generative approaches to achieve this
## Probabilistic Generative Models
- The goal is obtaining the class posterior using Bayes:
$$p(y =c\mid x) \propto {\color{#5D8AA8}{p(x\mid y = c)}} \cdot {\color{#FFA6C9}p(y=c)}$$
- The blue part is called the **class conditional**, meaning the probability of generating a point $x$, given that it belongs to class $c$
- The pink part is the **class prior** and represents the a priori probability of any point belonging to class $c$
### How it works
- A parametric model is chosen for the class conditional $p(x\mid y = c, \psi)$ and the prior $p(y=c\mid \theta)$ 
- In the **learning step**, the parameters $\{\psi, \theta\}$ are estimated using MLE, whereby $\{\hat \psi, \hat \theta\}$ are obtained
- Now, **inference** is possible, meaning a new $x$ can be classified
$$p(y=c\mid y, \hat \psi, \hat \theta) \propto p(x\mid y= c, \hat \psi)p(y=c\mid \hat \theta)$$
- In addition to generating class lables $y_{new}\sim p(y\mid \hat \theta)$, generative models can generate new feature vectors $x_{new}\sim p(x\mid y= y_{new}, \hat \psi)$ 
### Choosing the prior
- $y$ can take one of $C$ discrete values, distributed categorically
$$y\sim\text{Categorical(}\theta\text{)}$$
- $\theta \in \mathbb R^C$ specifies the probability of each class: 
$$p(y =c) = \theta_C \qquad \equiv \qquad  p(y) = \prod_{c = 1}^C\theta_c^{\mathbb I (y = c) }$$
- The MLE for $\theta$ is given by
$$\theta ^{MLE}_c = \frac 1N \sum_{i= 1}^N\mathbb I(y_i = c)$$
### Choosing class conditionals
- The feature vector $x\in \mathbb R^D$ is continuous, so we use a **multivariate normal** for each class
$$
p(x \mid y = c) = \mathcal N( x\mid \mu+c, \Sigma)
= \frac{1}{(2\pi)^{\frac D2}|\Sigma|^{\frac 12}}
\exp\left \{-\frac 12 (x - \mu_c)^T\Sigma^{-1}(x-\mu_c)\right \}
$$
- Since estimating all $\Sigma_c$s behaves badly numerically if not enough data is available, we use the same $\Sigma$ for all classes
### Posterior distribution
- We can now esimate the parameters for $p(x\mid y)$ and $p(y)$ as well as the training data parameters
- Since we have the parameters for $p(x\mid y)$ and $p(y)$ as well as the training data parameters, and given $\mathcal C = \{0,1\}$ we can use Bayes to arrive at the posterior
$$
\begin{align}
p(y = 1\mid x) &= \frac{p(x\mid y = 1) p(y = 1)}{p(x\mid y = 1) p( y =1)+ p(x \mid y = 0)p(y= 0)}\\
& = \frac{1}{1 + \exp(-a)}\\&=:\sigma(a)
\end{align}
$$
- This is equivalent to:
$$(y \mid \mathbf x) \sim \text{Bernoulli}(\sigma(\mathbf w^T \mathbf x + w_0))$$

![[Generative Models Posterior Visual.png|500]]
#### For $C > 2$
- If there are more than 2 classes, the following holds: 
$$p(y = c \mid \mathbf x) = \frac{p(\mathbf x \mid y = c) \cdot p(y = c)}{\sum_{c' = 1}^C p(\mathbf x \mid y = c') \cdot p(y = c')} = \frac{\exp(\mathbf w_c^T \mathbf x + w_{c0})}{\sum_{c' = 1}^C\exp(\mathbf w_{c'}^T \mathbf x + w_{c'0})}$$
#### Naive Bayes
- The class conditional distribution can be formed using naive Bayes
- We assume that the features $d$ of a sample are conditionally independent, whereby this holds: 
$$p(x_1,x_2, ..., x_d \mid y = c) = \prod_{i = 1}^d p(x_i \mid y = c)$$
- This approach results in quadratic boundaries
![[Naive Bayes.png|500]]
# Probabilistic Discriminative Models 
- Discriminative models directly model the posterior distribution $p(y \mid \mathbf x)$ 
## Logistic Regression
- The posterior distribution is modeled like so: 
$$(y\mid \x) \sim \text{Bernoulli}(\sigma (w^T\x + w_0)), \quad \text{with } \sigma(a)  =\frac{1}{1 + \exp(-a)}$$
- $w$ needs to be choosen such that the likelihood function is maximized
- For iid samples, the likelihood can be written as: 
$$p(\mathbf y \mid \mathbf w, \mathbf X) = \prod_{i = 1}^N p(y_i \mid \mathbf x_i, \mathbf w) = \prod_{i = 1}^N \sigma(\mathbf w^T \mathbf x_i)^{y_i} (1 - \sigma(\mathbf w^T \mathbf x_i))^{1 - y_i}$$
- We define a loss function called **binary cross entropy** 
$$E(\mathbf w) = -\ln(p(\mathbf y \mid \mathbf w, \mathbf X)) = - \sum_{i = 1}^N y_i \cdot \ln(\sigma(\mathbf w^T \mathbf x_i)) + (1 - y_i) \cdot \ln(1 - \sigma(\mathbf w^T \mathbf x_i))$$
- $\w$ needs to be chosen such that it minimizes $E(\w)$
