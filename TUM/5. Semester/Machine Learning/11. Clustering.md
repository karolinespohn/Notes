$\DeclareMathOperator{\x}{\mathbf x} \DeclareMathOperator{\y}{\mathbf y} \DeclareMathOperator{\z}{\mathbf z}\DeclareMathOperator{\w}{\mathbf w}\DeclareMathOperator{\b}{\mathbf b}\DeclareMathOperator{\W}{\mathbf W}\DeclareMathOperator{\a}{\mathbf a}\DeclareMathOperator{\dt}{\, \mathrm d t}\DeclareMathOperator{\dx}{\, \mathrm d x}\DeclareMathOperator{\dany}{\, \mathrm{d}}\DeclareMathOperator{\Var}{\text{Var}}\DeclareMathOperator{\V}{\mathbf V}\DeclareMathOperator{\diag}{\text{diag}}\DeclareMathOperator{\D}{\mathbf D}\DeclareMathOperator{\E}{\mathbb E}\DeclareMathOperator{\Q}{\mathbf Q}\DeclareMathOperator{\I}{\mathbf I}\DeclareMathOperator{\X}{\mathbf X}\DeclareMathOperator{\K}{\mathbf K}\DeclareMathOperator{\A}{\mathbf A}\DeclareMathOperator{\Z}{\mathbf Z}\DeclareMathOperator{\bftheta}{\boldsymbol{\theta}} \DeclareMathOperator{\bfSigma}{\mathbf \Sigma}\DeclareMathOperator{\bfmu}{\boldsymbol{\mu}} \DeclareMathOperator{\bfpi}{\boldsymbol{\pi}}$
# Introduction
- The aim of clustering is finding **latent structure** in unlabled data $\{\x_i\}$ 
- Given a collection of data points $\X = \{\x_1, ...\x_N\} \subset \mathbb X$, the goal is finding a corresponding assignment to one of $K$ groups $z_i \in \{1,...,K\}$, such that
- An assignment to a group $z_i \in \{1,...,K\}$ should be found for each $\x_i$, such that: 
	- Objects within the same group are similar
	- Objects in different groups are dissimilar
# $K$-Means Algorithm
- A **similarity function** $s(\cdot, \cdot): \mathbb X \times \mathbb X \to \mathbb R$ or a **distance function** $d(\cdot, \cdot)$ must be defined

- In the following, the $\mathbb R^D$ space with the L2 distance as a similarity function will be used
- Each cluster is defined by its **centroid** $\bfmu_K \in \mathbb R^D$ 
- A **cluster indicator** $\z_i \in \{0, 1\}^K$, where $\z_{ik}$ indicates if $\x_i$ belongs to cluster $k$
- The goal is minimizing this function with respect to the centroids $\bfmu$ and the cluster assignments $\Z$: 
$$J(\X, \Z,\bfmu) =\sum_{i = 1}\sum_{k = 1} z_{ik}||\x_i - \bfmu_k||_2^2 $$
$$\Z^*, \bfmu^* = \arg \min_{\Z, \bfmu} J (\X, \Z, \bfmu)$$
- The joint optimization is difficult to solve, so **alternating optimization** is applied on the following subproblems: 
$$\min_Z J(\X, \Z, \bfmu)  \qquad \min_{\bfmu}J(X, \Z, \bfmu)$$
## Lloyd's Algorithm
- Lloyd's Algorithm is used to solve $K$-Means: 
	1. Initialize the centroids $\bfmu = \{\bfmu_1,...,\bfmu_n\}$
	2. Update the cluster indicators by solving $\min_ZJ(\X, \Z, \bfmu)$ and updating the indicators: $z_i = \begin{cases}1 \quad \text{if }k = \arg \min_j ||\x_i - \bfmu_j||\\ 0 \quad \text{else}\end{cases}$
	3. Updating the centroids by solving $\min_\bfmu J(\X, \Z, \bfmu)$: $$\bfmu_k = \frac{1}{N_k}\sum_{i = 1}^N \z_{ik}\x_i, \qquad \text{where } N_k = \sum_{i = 1}^N\z_{ik} = \text{amount of data points}$$
	4. Return to step 2 if $J(\X, \Z, \bfmu)$ has not converged
![[clustering 2 clusters example.png|300]]
- However, this doesn't produce ideal results
- In this example, it would have been better to partition the red cluster into two than to separate the green and gray clusters
![[problems with clustering.png|300]]
- This problem stems from bad initialization of $\bfmu$
## $K$-Means++ Algorithm
- To choose the initial centroids, use the following algorithm: 
	 1. Choose the first centroid $\bfmu_1$ uniformly at random among the data points
	 2. Compute the distance $D_i^2 = ||\x_i - \bfmu_1||_2^2$ for each $\x_i$ 
	 3. Sample the next centroid $\bfmu_k$ from $x_i$ with a probability probortional to $D_i^k$. This way, points further away from centroid are prioritized
	 4. Recompute $D_i^2 = \min\{||\x_i - \bfmu_1||_2^2,..., ||\x_i -\bfmu_k||_2^2\}$
	 5. Repeat steps $3$ and $4$ until $K$ initial centroids are chosen
## Limitations
- The **model** (distortion of $J(\X, \Z, \bfmu)$)  cannot detect **overlapping convex hulls** (see image), is sensitive to **outliers** and has **no uncertainty measure**, but just hard decision boundaries
![[overlapping convex hulls.png|300]]
- The **algorithm** is extremely sensitive to initialization
# Gaussian Mixture Models
- The goal is designing a **probabilistic model** for the data $p(\x, \mid \theta)$ 
- To create such a model,  we assume that each point $\x$ belongs to some cluster $z$ and instead model $p(\x, \z \mid \theta)$
## Estimating $\theta$
- Since we use unsupervised learning, we need to estimate $\bftheta$ given $\X$, without knowing $\Z$ 
$$p(\x\mid\bftheta) = \sum_\Z p(\X, \Z\mid \bftheta) = \sum_\Z p(\X\mid\Z, \bftheta) \cdot p(\Z\mid\bftheta)$$
- Next, we maximize the function with regard to $\bftheta$
$$\bftheta^* = \arg\max_\bftheta p (\X\mid\bftheta)$$
- The **prior** $p(z \mid \bftheta)$ is modeled as categorical distribution: 
$$p(\z\mid\bftheta) = \text{Cat}(\bfpi) $$
- Each cluster has its own **multivariate normal distribution**: 
$$p(x\mid z_k = 1, \theta) = \mathcal N(\x, \boldsymbol{\bfmu}_k, \Sigma_k) $$
- The parameters inclueded in $\bftheta$ are:  $\bftheta =\{\bfpi, \boldsymbol{\bfmu}, \bfSigma\}$ 
## Generative GMM
- Given $K$ Gaussian clusters with known $\{\boldsymbol{\bfmu}_k, \bfSigma_k\}$ and the prior probabilities $\bfpi_k$, we can generate a data point $\x$ like this: 
	- A one-hot cluster indicator $\z \sim \text{Cat}(\bfpi)$ is drawn, where $\z = 1 \Leftrightarrow$ current point belongs to cluster $k$ 
	- The sample $\x$ is drawn with $\x\sim\mathcal N(\bfmu_k, \bfSigma_k)$ if $z_k = 1$
- The probability of a single sample $\x$ under a GMM can be computed like this: 
$$
\begin{align}
p(\x\mid \bfpi, \bfmu, \bfSigma) &= \sum_{k = 1}^Kp (\z_k = 1\mid \bfpi)\cdot p(\x\mid\z_k = 1, \bfmu_k, \bfSigma_k) \\&
= \sum_{k = 1}^K\bfpi_k \cdot \mathcal N(\x\mid\bfmu_k, \bfSigma_k)
\end{align}
$$
- The $\log$-likelihood for $\X$ is given by:
$$\log p(\X\mid\bfpi, \bfmu, \bfSigma) = \sum_{i = 1}^N \log\left(\sum_{k = 1}^K\bfpi_k \cdot \mathcal N(\x_i \mid \bfmu_k , \bfSigma_k)\right)$$

![[gmms.png|400]]
## Learning and Inference
- In GMM, the parameters $\bfmu^*, \bfpi^*$ and $\bfSigma^*$ are first learned, and then the correct class can be infered
- However, understanding the solution is easier if we first look at inferene
### Inference
- In the **inference** step, we have already estimated the parameters and now want to assign the points to clusters
$$p(\Z\mid\X, \bfpi, \bfmu, \bfSigma)$$
- The posterior is called **responsibility** and it is given by:
$$
\begin{align}
\gamma(\z_{ik}) :&= p (\z_{ik} = 1\mid \x_i \bfpi, \bfmu, \bfSigma) = \\\\
& = \frac{\bfpi_k \mathcal N (\x_i\mid\bfmu_k, \bfSigma_k)}{\sum_{j = 1}^{K} \bfpi_j\mathcal N(\x_i\mid \bfmu_j, \bfSigma_j)}
\end{align}

$$
![[Screenshot 2025-01-22 at 15.18.44.png|400]]
### Learning
- To learn the parameters $\bfmu^*, \bfpi^*$ and $\bfSigma^*$, the $\log$-likelihood needs to be maximized
$$
\begin{align}
\bfmu^*, \bfpi^*, \bfSigma^* &= \arg \max_{\bfpi, \bfmu, \bfSigma} \log(p(\X\mid \bfpi, \bfmu, \bfSigma))\\
&= \arg \max_{\bfpi, \bfmu, \bfSigma} \sum_{i = 1}^N \log\left(\sum_{k = 1}^K\bfpi_k \cdot \mathcal N(\x_i \mid \bfmu_k , \bfSigma_k)\right)
\end{align}
$$
- Since the normal distribution is in the sum, the $\log$ doesn't act on it
- We cannot find analytical solutions for the parameters, but we now how to solve this problem: 
$$\bfmu^*, \bfpi^*, \bfSigma^* = \arg \max_{\bfmu, \bfpi, \bfSigma}\log p(\X, \Z\mid\bfmu, \bfpi, \bfSigma)$$
- We don't know $\Z$, but we can use the **initial beliefs** $\gamma_0(\Z) = p (\Z\mid\bfmu^{(0)}, \bfpi^{(0)}, \bfSigma^{(0)})$, perhaps determined through $K$-Mean, and solve for $\bfmu^*, \bfpi^*$ and $\bfSigma^*$ with that knowledge
- Instead of computing $p(\X)$, we can formulate an easy to solve **proxy-objective**, by introducing the latent variables $\Z$  and taking their **expectation**
$$\mathbb E_{\Z \sim \gamma_t(\Z)}[\log p(\X, \Z\mid\bfpi, \bfmu, \bfSigma)]$$
#### EM-Algorithm
- The **expectation-maximum** algorithm repeatedly evaluates $\gamma_t(\Z)$ with respect to the current estimates $\bfmu_t, \bfpi_t$ and $\bfSigma_t$, and calculating the updated parameters
$$\bfpi^{(t + 1)}, \bfmu^{(t +1)}, \bfSigma^{(t + 1)}  = \arg\max_{\bfpi, \bfmu, \bfSigma} \; \mathbb E_{\Z \sim \gamma_t(Z)}\;[\log p(\X, \Z\mid \bfpi, \bfmu, \bfSigma)]$$
- The whole algorithm goes like this: 
1. Initialize the model parameters $\{\bfpi^{(0)}, \bfmu_1^{(0)},...,\bfmu_K^{(0)}, \bfSigma_1^{(0)}, ...\bfSigma_K^{(0)}\}$
2. **E Step**: Evaluate the responsibilities: $$\gamma_t(z_{ik}) = \frac{\bfpi_k^{(t)} \mathcal N (\x_i\mid\bfmu_k^{(t)}, \bfSigma_k^{(t)})}{\sum_{j = 1}^{K} \bfpi_j^{(t)}\mathcal N(\x_i\mid \bfmu_j^{(t)}, \bfSigma_j^{(t)})}$$
3. **M Step**: Re-estimate the parameters: $$\begin{align}\bfmu_k^{(t + 1)} &= \frac {1}{N_k}\sum^{N}_{i = 1}\gamma_t(\z_{ik})\x_i\\ \bfSigma_k^{(t + 1)} & = \frac{1}{N_k}\sum^{N}_{i = 1}\gamma_t(\z_{ik})(\x_i - \bfmu_k^{(t + 1)})(\x_i - \bfmu_k^{(t +1)})^T\\ \bfpi_k^{(t + 1)} &=\frac{N_k}N \qquad \text{where } N_k = \sum_{i = 1}^N \gamma_t(\z_{ik}) \end{align}$$
4. Iterate steps 2 and 3 until $E_{\Z \sim \gamma_t(Z)}\;[\log p(\X, \Z\mid \bfpi^{(t)}, \bfmu^{(t)}, \bfSigma^{(t)})]$ converges
## Choosing $K$
### Heuristic Methods
- The **elbow/knee** heuristic plots within-cluster sum of squared distances for varying $K$, and looks for a bend
- **Gap statistics** compare in cluster variation to uniform data and choose the smallest $K$ such that the gap statistic is within one standard deviation
- **Silhouette**: Todo
### Probabilistic methods
- For probabilistic models, a generative model $\hat L= p(\X\mid\hat\Z, \hat \bftheta)$ with optimal $\hat \Z$ and $\hat \bftheta$ is needed
- **BIC:** Approximates the model likelihood $p(\X\mid \text{model})$, balancing the amount of parameters and the likelihood: 
$$\text{BIC} = M\log N- 2 \log \hat L$$
- $M$ is the number of free parameters, and $N$ is the number of samples
