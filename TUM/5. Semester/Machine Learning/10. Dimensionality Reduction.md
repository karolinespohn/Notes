$\DeclareMathOperator{\x}{\mathbf x} \DeclareMathOperator{\y}{\mathbf y} \DeclareMathOperator{\z}{\mathbf z}\DeclareMathOperator{\w}{\mathbf w}\DeclareMathOperator{\b}{\mathbf b}\DeclareMathOperator{\W}{\mathbf W}\DeclareMathOperator{\a}{\mathbf a}\DeclareMathOperator{\dt}{\, \mathrm d t}\DeclareMathOperator{\dx}{\, \mathrm d x}\DeclareMathOperator{\dany}{\, \mathrm{d}}\DeclareMathOperator{\Var}{\text{Var}}\DeclareMathOperator{\V}{\mathbf V}\DeclareMathOperator{\diag}{\text{diag}}\DeclareMathOperator{\D}{\mathbf D}\DeclareMathOperator{\E}{\mathbb E}\DeclareMathOperator{\Q}{\mathbf Q}\DeclareMathOperator{\I}{\mathbf I}\DeclareMathOperator{\X}{\mathbf X}\DeclareMathOperator{\K}{\mathbf K}\DeclareMathOperator{\A}{\mathbf A}\DeclareMathOperator{\bfSigma}{\mathbf \Sigma}$
# Introduction
- While [[TUM/5. Semester/Machine Learning/1. Introduction|supervised Learning]] is concerned with mapping inputs to targets, [[TUM/5. Semester/Machine Learning/1. Introduction|unsupervised learning]] can be viewed as **modelling**
- The aim is finding hidden structure in data
- Common use cases are **clustering** and **anomaly detection**
- It can be used as a standalone method, for preprocessing and for pretraining
- Unsupervised learning can serve as a mechanism for **compression**: 
	- Data points are compressed to a label matching its cluster
	- Data points can be compressed to a lower dimension 
- Lowering the dimesion while avoiding information loss is desirable since it lowers the cost of computation and counteracts the curse of dimensionality
## Feature Sub-Selection
- Using a priori knowledge, dimensions with comparatively low entropy can be removed
- **Low variance** dimensions and dimensions **irrelevant** to the problem are often good choices
- This requires little preprocessing, but expert knowledge is required and misjugement is possible
### Transforming Axes
![[Low variance dimension vs no low var dim.png|400]]
- In the left graph, the dimensions, the lower variance feature $x_2$ can be reduced to just $x_1$
- The right graph is simply a rotation of the left graph, but neither dimension has a low variance 
- Such graphs occur frequently since features are often correlated
 - To solve this, the coordinate axes can be rotated
 ![[rotation of coordinate system.png|400]]
 - An  can be used like so: 
- Using an **orthonormal transformation** matrix $\F \in \mathbb R^{d \times k}$, a column $\x$ can be transformed into a new coordinate system defined by $\F$ like this: 
$$(\x')^T = \x^TF$$
- A matrix $\X$ is transformed as follows: 
$$(\X')^T = \X^T\F$$
![[orthogonal transformation.png|200]]
### Linear Transformations
- **Feature selection** can be written as a matrix $\F$
- To select the $2$nd and $4$th feature out of a $5$ dimensional vector, a matrix can be defined like this: 
$$\F= \begin{bmatrix}
0 & 0 \\ 1 & 0 \\ 0 & 0 \\ 0 & 1 \\ 0 & 0 \\
\end{bmatrix}$$
# Principal Component Analysis
- The goal for $\F$ is to transform the data in such a way that the is **linearly uncorrelated**
- $\F$ is optimal if the data is approximated as well as possible with few coefficients
- In PCA, a coordinate system is found in which the originally correlated points are linearly uncorrelated
- The dimensions with low variance are then ignored
## Approach
- Given $N$ $d$-dimensional data points, $\{\x_i\}_{i = 1}^{N}\x_i, \x_i \in \mathbb R^d, \quad \forall i \in \{1,...,N\}$, we represent the points by a matrix $\X$
$$\X= \begin{bmatrix}
x_{11} & \cdots & x_{1d}\\
\vdots & \ddots & \vdots\\
x_{N1} & \cdots & x_{Nd}\\
\end{bmatrix}$$
- The row $\x_i \in \mathbb R^d$ denotes the $i$th point and the column $\X_{:,j}$ denotes the $j$th column containing all values in the $j$th dimension
- To achieve a covariance of $0$ between the new dimensions, the following steps are made: 
1. The data is **centered**
2. The **covariance matrix** is computed
3. **Eigenvector decomposition** is used to transform the coordinate system 
![[Eigen Decomposition.png|400]]
### Centering Data
- The data is centered by shifting their points by their mean $\overline \x \in \mathbb R^d$: 
$$\tilde\x_i = \x_i - \overline \x$$
![[Centering Data.png|300]]
#### Statistical Foundations
- **First order statistics** refers to the number of points $N$
- **Second order statistics** refers to the mean $\overline \x \in \mathbb R^d$ of $N$ points: 
$$\overline x = \begin{bmatrix}
\overline \x_1 \\
\vdots \\
\overline \x_d \\
\end{bmatrix} = \frac 1N\cdot \X^T\cdot \mathbf 1_N$$
- $\mathbf 1_N$ denotes an $N$ dimensional vector of ones
### Compute the Covariance Matrix
- $\Var(\tilde \X_j)$ needs to be determined for each dimension $j \in \{1,...,d\}$
- The covariance $\Cov(\tilde \X_{j_1}, \X_{j_2})$ between dimensions $j_1, j_2$,  $\forall j_1 \neq j_2 \in \{1,...,d\}$ needs to be determined
#### Statistical Foundations
- **Second order statistic** refers to variance and covariance
$$\Var(\X_j) = \frac 1N \sum_{i = 1}^N (\x_{ij}-\overline\x_j)^2 =\frac 1N\X_j^T\X_j-\overline\x_j^2$$
$$\begin{align}
\Cov(\X_{j_1}, \X_{j_2}) &= \frac1N\sum_{i = 1}^N(\x_{ij_1}-\overline \x_{j_1})(x_{ij_2}- \overline \x_{j_2}) = \frac 1N\X_{j_1}^T\X_{j_2} - \overline \x_{j_1}\overline\x_{j_2} \\
&=\frac{1}{N}\X^T\X- \hat\X\hat \X
^T\end{align}$$
### Eigen-Decomposition
- The next step is computing the eigenspectral decomposition (factorization) of the covariance matrix:
$$\mathbf \Sigma_{\hat X}= \Gamma \cdot\Lambda \cdot \Gamma^T$$
- The matrices $\Gamma, \Lambda \in \mathbb R^{d\times d}$ 
- $\Gamma$ is contains all normalized eigenvectors $\gamma_i$ of the covariance matrix and it is orthogonal: 
$$\Gamma \cdot \Gamma^T = \Gamma^T\cdot\Gamma = \text{Id}(\Gamma^T = \Gamma^{-1})$$
- $\Lambda$ is a diagonal matrix with the eigenvalues $\lambda_i$ as diagonal elements
- The **new coordinate system** is defined by the eigenvectors $\gamma_i$ and the transformed data is given by $\Y = \tilde \X \cdot \Gamma$ 
- $\Lambda$ is the new covariance matrix
- This means that the system has variance $\lambda_i$ in dimension i and $\forall i_1 \neq i_2: \Cov(\Y_{i_1}, \Y_{i_2}) = 0$
## Dimensionality Reduction with PCA
- Coordinates with low variance ($\lambda_i$) can be ignored
- $\Gamma$ is truncated such that only the columns of the largest $k$ eigenvalues $\lambda_1,...,\lambda_k$ are kept
$$\Y_{\text{reduced}} = \tilde \X \cdot \Gamma_{\text{truncated}}$$
- Frequently, $k$ is chosen such that the $k$ variances explain $90\%$ of the energy:
$$k = \text{smallest value ensuring} \sum_{i = 1}^k \lambda_i \ge 0.9 \cdot \sum_{i = 1}^d \lambda_i$$
## Complexity
- The complexity is very high: 
$$\underbrace{\mathcal{O}(N\cdot d^2)}_{\substack{\text{Compute covariance } \\ \text{matrix}}} + \underbrace{\mathcal O(d^3)}_{ \substack{
\text{Eigenvalue} \\ \text{Decomposition} } } + 
\underbrace{\mathcal O (N\cdot d \cdot k )}_{
\substack{ \text{Projection onto $k$} \\ \text{dimensional space}
} } = \mathcal O(N \cdot d^2 \cdot d^3)$$
### Power Iteration
- However, we actually do not need all eigenvectors but just the $k$ largest ones
- Iterative approaches can be used, which reduce the complexity to $\mathcal O(k \cdot d^2)$ or for sparse matrices even $\mathcal O(k \cdot \# nz)$ with $\#nz=$ number of nonzero elements

- One approach for numerically calculating the eigenvector corresponding to the largest eigenvalue is **power iteration**
- Iteratively compute
$$v \to \frac{\A\cdot v}{||\A\cdot v||}$$
- The **convergence rate** is $|\frac{\lambda_2}{\lambda_1}|$
#### Finding more eigenvectors
- For a symmetric matrix $\A$, eigenvalue decomposition leads to
$$\A = \Gamma \Lambda \Gamma^T = \sum_{i = 1}^d\lambda_i\cdot \gamma_i\gamma_i^T$$
- The deflated matrix $\tilde \A= \A-\lambda_1\cdot \gamma_1\cdot\gamma_1^T$
- To find the second largest eigenvector, power iteration is applied to $\tilde \A$
## PCA vs Regression
- While PCA is conceptionally similar to regression, the results differ
![[PCA vs Regression.png|500]]
- In the first image  (PCA), we minimize the euclidian distance of the points to the new axis
- In the 2nd and 3rd image (regression) we minimize the horizontal and vertical distance respectively from the points to the axes 
# Singular Value Decomposition
- Often times, the data lies on a low-dimensional manifold in higher dimensional space
![[low dimensional manifolds.png|300]]
## Rank of a Matrix
- The rank of a matrix is the number of **linearly independant** columns/rows
- The rank of the following matrix is $2$: 
$$\begin{pmatrix} 1 & 2 & 1 \\-2 & -3 & 1 \\ 3 & 5 &0 \end{pmatrix}$$
- The first two rows are linearly independant, but all rows are linearly dependant, since the first row is equivalent of the sum of the second and the third row
- This means, $\A$ can be written as: 
	- the basis vectors $\begin{pmatrix}1 & 2 & 1\end{pmatrix}, \begin{pmatrix}-2 & -3 &1\end{pmatrix}$ 
	- The new coordinates: $\begin{pmatrix}1 & 0\end{pmatrix}, \begin{pmatrix}0 & 1\end{pmatrix}, \begin{pmatrix}1 & -1\end{pmatrix}$
- The number of coordinates was reduced from $3$ to $2$
### Low Rank Approximation
- Matrices can sometimes be approximated by lower rank matrices:
$$\A = \begin{pmatrix}
1.01 & 1.05 & 0.9\\-2.1 & -3.05 & 1.1 \\ 2.99 & 5.01 & 0.3
\end{pmatrix}\approx\begin{pmatrix} 1 & 2 & 1 \\-2 & -3 & 1 \\ 3 & 5 &0 \end{pmatrix}=\B$$
 - In this example, $\rank(A) = 3$ but $\rank(\B)=2$ 
  - Given a matrix $\A \in \mathbb R^{n \times d}$, find $\B \in \mathbb R^{n \times d}$ with $\rank(\B) = k$, such that the following term is minimized: 
$$||\A-\B||_F^2 = \sum_{i = 1}^N\sum_{j = 1}^D(a_{ij} -b_{ij})^2 \qquad \text{subject to }\rank(\B) = k$$
- The square difference per element is minimized
## Definition SVD
- Every real matrix $\A \in \mathbb R^{n \times d}$ can be decomposed into: 
$$\A = \U \cdot \bfSigma \cdot \V^T = \sum_{i = 1}^r\sigma_i\cdot \u_i \cdot \v_i^T$$
- Where: 
	- $\U \in \mathbb R^{n \times r}, \quad \bfSigma \in \mathbb R^{r \times r}, \quad \V \in \mathbb R^{d \times r}$
	- $\U$ and $\V$ are column orthonormal: 
	$$\U^T\U = \I, \quad \V^T\V = \I$$
	- $\bfSigma \in \mathbb R^{r \times r}$ is a diagonal matrix and $r =  \rank(\A)$
	- The entries of $\bfSigma$ are **singular values**, sorted in decreasing order: $\sigma_1 \ge \sigma_2 \ge...\ge 0$
	- The decomposition is almost unique, except for the sign
![[singular value decomposition.png|300]]
![[svd sums.png|300]]
### Example
![[svd example.png|400]]
- It is possible to instead write this with the signs of the second column and row of $\U$ and $\V$ respectively flipped:
![[svd example -1.png|400]]
- Instead of 5 dimensions, we can identify $2$ concepts and reduce the dimensions to $2$ dimensions
### Interpretation
- Usually, the data is not quite as perfect as the matrix above, but it may look more like this 
![[svd with noise.png|400]]
- The original data $\A$ is a similarity matrix of "movies-to-users"
- The matrix $\U$ can be viewed as a "user-to-concept" similarity matrix
	- The first two columns represent sci-fi and romance, the last column represents the data in the "Alien" column
- The matrix $\bfSigma$ marks the strength of each concept: 
	- Sci-fi and romance are strong concepts, but the third concept is not strong
- $\V$ can be viewed as a "movie-to-concept" similarity matrix
## Benefits of SVD
- SVD allows discovering hidden correlations and topics
- It allows for interpretation and visualization
## Reducing Dimensionality
- The singular values give the strengths of the concepts
- We can reduce the dimensionality while getting the best approximation by setting the smallest singular value to $0$
![[svd reduction.png|400]]
- By construction, the approximation on the right has $\rank = 2$
![[svd reduction approximation.png|400]]
![[svd approximation.png|400]]
The **projected/reduced** data can be obtained by computing: 
$$P = \U \cdot \bfSigma \equiv \A\cdot\V$$
# Matrix Factorization 
![[netflix rating prediction example.png|300]]
- Given a matrix of movies and user ratings, the goal is to predict the remaining errors
- **RMSE** can be used as a quality measurement given a set of tuples $S = (u, i)$ of users $u$ and that rated item $i$ with $r_{ui}$
$$\RMSE = \sqrt{\frac{1}{|S|}\sum_{(u, i)\in S}(r_{ui}-\hat r_{ui})^2} \qquad \text{where } r_{ui} = \text{actual rating and } \hat r_{ui}=\text{predicted rating} $$
### Computing Missing Values
#### Naive Idea
- One idea of how to get the actual ratings is using SVD: compute $Q \cdot P^T$, and thereby approximate $R$
![[svd recommender systems.png|500]]
- Since $R$ is sparce, the matrix approximated by $Q\cdot P^T$ will approximate a lot of values to be close to $0$
##### Missing Entries
- This problem occurs because missing entries are initialized to $0$, and not treated differently from $0$-ratings
- We can update our goal to be: 
$$\min_{P, Q}\sum_{(u, i) \in S}(r_{ui}-\mathbf q_u\cdot\mathbf p_i^T)^2$$
- Now, we only sum over existing entries: 
$$S = \{(u,i)\mid r_{ui}\neq \text{missing}\}$$
- $P$ and $Q$ are not required to be orthogonal/unit length
#### Alternating Optimization
- One algorithm for finding $P$ and $Q$  is **Alternating Optimization**
- $P$ and $Q$ are set to initial values and alternatingly one is fixed and the other is optimized
- This process is repeated until convergence
``` 
Initialize P_0, Q_0, t = 0
while (not convergence){
	P_(t + 1) = arg min_P f(P, Q_t)
	Q_(t + 1) = arg min_Q f(P_(t + 1), Q)
	t++
}
```
##### Initialization
- $P$ and $Q$ can be initialized such that all missing entries are $0$ or the overall mean
##### Finding $P$ and $Q$
$$P^{(t + 1)} = \arg \min_P f(P, A^{(t)})= \arg \min _P \sum_{(u, i) \in S}(r_{ui}-\mathbf q_u\cdot\mathbf p_i^T)^2$$
- $Q$ is fixed, therefore the problem can be soved independently for each $p_i$
$$\sum_{i=1...d}\min_{p_i}\sum_{u \in S_{*, i}}\left(r_{ui}-q_u\cdot p_i^T\right)^2 \qquad \text{with }S_{*. i} = \{u\mid (u, i) \in S\}$$
##### Discussion
- The implementation is easy and the algorithm is efficient
- However, the solution is an approximation and there are no guarantees that the solution is close to the actual solution
#### Stochastic Gradient Descent for Matrix facrorization
- The idea is performing stochastic gradient descent on the objective $\mathcal L := \sum_{(u, i) \in S}(r_{ui}-\q_u\cdot \p^T_i)^2$
	- A random user $u$ and a random item $i$ with rating $r_{ui}$ is selected
	- The gradients $\frac{\partial \mathcal L}{\partial \q_u}$ and $\frac{\partial \mathcal L}{\partial \p_i}$ are computed
	- The parameters are updated with learning rate $\eta$: $\q_u \leftarrow \q_u-\eta \frac{\partial \mathcal L}{\partial \q_u},  \p_i  \leftarrow \p_i-\eta \frac{\partial \mathcal L}{\partial \p_i}$
- The actual update step looks like this: 
$$\begin{align}
e_{ui} & \leftarrow r_{ui} - \q_u \cdot \p_i^T \qquad \text{(helper variable)} \\\\
q_{u} & \leftarrow q_{u} - 2\eta (e_{ui}\p_i)\\\\
p_{i} & \leftarrow p_{i} - 2\eta (e_{ui}\q_u)
\end{align}$$
##### Claculating a Missing Entry
- The missing entry rating can be predicted for user $u$ for item $i$ like so: 
$$\hat r_{ui} = \q_u \cdot \p_i^T = \sum_k\q_{uk}\cdot p_{uk}$$
- $\q_u$ refers to the $u$th row of $\Q$ and $\p_i$ refers to the $i$th row of $P$
![[sdg recommender systems.png|400]]
### Biases
- Some users may be overly optimistic or pessimistic and some movies may always have high or low ratings
- For such cases, bias terms can be introduced: 
	- Each user $u$ can have a bias $b_u$ 
	- Each item can have a bias $b_i$
	- Global bias terms $b$ can be introduced
- The optimization problem including biases goes like this: 
$$\min_{\stackrel{\P, \Q}{b_u, b_i, b}}\; \sum_{(u, i) \in S}(r_{ui} - (\q_u\cdot \p_i^T + b_u + b_i + b))^2$$
### Regularization 
- Overfitting becomes a problem, when the number of regression parameters/latent factors $k$ is too large
- To avoid such issues **regularization** is employed
	- If **sufficient data** is available, the model is allowed to be rich
	- If data is **sparse**, parameter vectors are shrunk aggressively
$$\min_{\P, \Q}\underbrace{\sum_{(u, i)\in S}\left(r_{ui} - \q_u\cdot \p^T_i\right)^2}_{\text{error}}+ \left[\underbrace{\lambda_1 \sum_u||
q||^2 + \lambda_2\sum_{i}||p||^2}_{\text{length}}\right]$$
- $\lambda_{1, 2}$ are user defined regularization parameters
- The regularized version of alternating optimization becomes ridge regression
$$\min_{\p_i}\sum_{i\in S_{*, i}}\left(r_{ui} - \q_u\cdot \p^T_i\right)^2 + \lambda_2||p_i||^2$$
#### L2 vs. L1 Regression
- **L2 regression** tries to shrink the parameter vector $w$ equally
	- Due to the ${}^2$, large data is heavily penalized and small data is hardly penzlized
	- Parameters will likely never be exactly $0$
- **L1 regression** allows large values in components by shrinking other components to $0$
	- Sparsity can be created
# Neighbor Graph Models
- Sometimes, data is structured in a way that is lost when performing PCA
![[pc local neighborhoods.png|300]]
- An idea for perserving structures is using **neighbor classes**
![[neighborhood graphs.png|400]]
- The goal is finding a lower dimension, where data close together in a higher dimension is still close together in the lower dimension
## tSNE
### Higher Dimensional Neighbors
- The following equations are used to model the probability of $x_i$ and $x_j$ being neighbors
$$p_{i\mid j} = \frac{\exp\left(-\frac{||x_i - x_j||^2}{2\sigma_i^2}\right)}{\sum_{k \neq i, k \in N} \exp\left(-\frac{||x_i-x_k||^2}{2\sigma_i^2}\right)}$$
$$p_{ij} = \frac{p_{i \mid j} + p_{j \mid i}}{2N}$$
$$p_{ii} = 0$$
- The $\sigma_i$s indicate the neighborhood radius/how many neighbors will be found 
### Lower Dimensional Neighbors
- In the lower dimensional latent space, we measure similarity between the latent vectors $y_i$ and $y_j$ like so: 
$$q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_k \sum_{l \neq k}(1 + ||y_k - y_l||)} \qquad q_{ii} = 0$$
### Minimizing difference
- In the next step, $p_{i\mid j}$ and $q_{i, j}$ are optimized, such that they are as similar as possible, thereby retaining neighbors
- To that effect, the KL divergence of the distributions is minimzied
$$\min_{y_i}KL(P||Q) = \sum_i\sum_{j \neq i}p_{ij}\cdot \log\left(\frac{p_{ij}}{q_{ij}}\right)$$
#### KL Divergence
$$KL(P||Q) \ge 0 \qquad\forall P, Q$$
$$KL(P||Q) = 0 \Leftrightarrow P = 0$$
- The KL Divergence is not symmeyric: $KL(P||Q) \neq KL(Q||P)$
![[KL Divergence.png|400]]
### Examples
![[tSNE.png]]
# Autoencoders
- Autoencoders provide a way for **nonlinear dimensionality reduction**
- An autoencoder is a **neural network** that finds a compact representation of data by learning to reconstruct it
$$f(\x, \W) := \hat \x \approx \x$$
## Approach
 - The aim is to minimize the reconstruction error between $\hat \x$ and $\x$
 $$\min_\w \frac 1N\sum_{i = 1}^N ||f(x_i, \W) - x_i||^2$$
 - An alternative view is, that the goal is finding a latent representation $\z \in \mathbb R^L$ for $x \in \mathbb R^D$ with $L \ll R$
 - A neural network implements $f_{enc}$ and $f_{dec}$ 
$$f_{enc}(\x) = z \qquad f_{dec}(z) \approx \x $$

- The output has **fewer neurons** than the input
 ![[autoencoder.png|400]]
## Practical Information
- Autoencoders where $L \ll D$ are called **undercomplete** and they force the autoencoder to filter out the most important features
+ To prevent overfitting, a regularization term is used 
+ The weights of $f_{enc}$ and $f_{dec}$ are **tied**, meaning they share the weights
- Autoencoders can be used for **denoising** data
- 
## Example 
 ![[Autoencoder 2 example.png|400]]
 ![[autoencoder numbers example.png|400]]
 