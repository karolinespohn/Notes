$\DeclareMathOperator{\x}{\mathbf x} \DeclareMathOperator{\y}{\mathbf y} \DeclareMathOperator{\w}{\mathbf w}\DeclareMathOperator{\b}{\mathbf b}\DeclareMathOperator{\W}{\mathbf W}\DeclareMathOperator{\a}{\mathbf a}\DeclareMathOperator{\dt}{\, \mathrm d t}\DeclareMathOperator{\dx}{\, \mathrm d x}\DeclareMathOperator{\dany}{\, \mathrm{d}}\DeclareMathOperator{\Var}{\text{Var}}\DeclareMathOperator{\V}{\mathbf V}\DeclareMathOperator{\diag}{\text{diag}}\DeclareMathOperator{\D}{\mathbf D}\DeclareMathOperator{\E}{\mathbb E}\DeclareMathOperator{\Q}{\mathbf Q}\DeclareMathOperator{\I}{\mathbf I}\DeclareMathOperator{\X}{\mathbf X}\DeclareMathOperator{\K}{\mathbf K}\DeclareMathOperator{\A}{\mathbf A}$
# (Hard Margin) Support Vector Machines
- In linear classification, having a wide margin around a classifier is desirable 
![[svm bad hyperplane.png|300]]![[svm good hyperplane.png|300]]
- The goal is finding a hyperplane that separates both classes with the maximum possible margin
## Maximum Margin Classifiers
- We add 2 more hyperplanes, between which no points may lie: 
$$\w^T \x + (b âˆ’ s) > 0 \quad \forall x \text{ in blue class}$$
$$\w^T \x + (b + s) > 0 \quad \forall x \text{ in green class}$$
![[svm margins.png|300]]
- The signed distance from the origin to the hyperplane is given by: 
$$d = -\frac{b}{||\w||}$$
- Therefore the distances to the blue and green hyperplane are given by:
$$d_{\color{lightblue}{blue}} = -\frac{b- s}{||\w||} \qquad d_{\color{lightgreen}{green}} = -\frac{b + s}{||\w||}$$
### Margin
- The margin is: 
$$m = d_{\color{lightblue}{blue}} - d_{\color{lightgreen}{green}} = \frac{2s}{||\w||}$$
- $s$ can be set to $1$, because $\w$ can be scaled without affecting the optimization
$$m = \frac{2}{||\w||}$$
- Let the $i$th sample $x_i$ be assigned to $y_i\in\{-1,1\}$, and given these constraints:  
- If the following are fulfilled, the margin is $m = \frac{2}{||\w||} = \frac{2}{\sqrt{\w^T\w}}$
$$\w^T \x_i+ b\ge 1 \qquad \w^T \x_i+ b\le -1$$
$$y_i(\w^T\x_i + b) \ge 1 \quad \forall i$$
## Optimizstion With Inequality Constraints
- To find the optimal hyperplane, we need to find $\w$ and $b$ such that: 
$$f_0(\w, b) = \frac 12 \w^T\w \quad \text{is minimized}$$
$$\text{with the constraint }\quad f_i(\w, b) = y_i (\w, b) -1 \ge 0 \quad \text{for } i =1,...,N$$
- This optimization problem is **convex** and **constrained**
- Given the functions $f_0: \mathbb R^d \to \mathbb R$ and $f_i: \mathbb R^d \to \mathbb R$
- Given the task: Minimize $f_0(\theta)$ subject to $f_i(\theta) \le 0$ for $i = 1,...,M$
### Definitions
- A point $\theta$ is called **feasible** , $\Leftrightarrow$ it satisfies the constraints of the optimization problem: 
$$f_i(\theta) \le 0 \quad \forall i \in \{1,...,M\}$$
- The optimal value is called the **minimum** $p^*$ and the point where $p^*$ is obtained is the minimizer $\theta^*$
$$f_0(\theta^*) = p^*$$
- The **Lagrangian** $L: \mathbb R^d \times \mathbb R^M \to \mathbb R$ is defined by: 
$$L(\theta, \alpha) = f_0(\theta) + \sum_{i = 1}^M\alpha_if_i(\theta)$$
- $\alpha_i \ge 0$ is the **Lagrange Multiplier** associated with the inequality constraint $f_i(\theta) \le 0$ 
- The (concave) **Lagrange dual function** is defined as: 
$$g(\alpha) = \min_{\theta \in \mathbb R^d}L(\theta, \alpha) = \min_{\theta \in \mathbb R^d}\left(f_0(\theta)+ \sum_{i =1}^Ma_if_i(\theta)\right)$$
### Bounds
- For every $\alpha$, $g(\alpha) = \min_{\theta \in \mathbb R^d} L(\theta, \alpha)$  is a lower bound for $p^*$
$$\min_{\stackrel{\theta \in \mathbb R^d}{f_i(\theta) \le 0}} f_0(\theta^*) \ge f_0(\theta^*) + \sum_{i = 1 }^M\underbrace{\alpha_i}_{\ge 0}\underbrace{f_i(\theta^*)}_{\le 0} = L(\theta^*, \alpha) \ge \underbrace{\min_{\theta \in \mathbb R^d}\{L(\theta, \alpha)\}}_{g(\alpha)}$$
- Therefore $f_0(\theta^*) \ge g(\alpha)$ 
- The best (highest) lower bound we can achieve can be found by maximizing $g(\alpha)$ subject to $\alpha_i \ge 0, \quad i =1,...,m$
- The maximum $d^*$ of the Lagrange dual function is the best lower bound we can find for $p^*$
### Complementart Slackness
- When $d^* \le p^*$ we have **weak duality** and if $d^* = p^*$ we have **strong duality**
- **Weak duality** means $d^* \le p^*$ and **strong duality** means $d^* = p^*$
- Given strong duality, **complementary slackness** holds:
$$\begin{align}
f_0(\theta^*) & = g(\alpha^*) &\\
& = f_0(\theta^*) + \sum_{i = 1}^M \alpha_i^*f_i(\theta^*) \quad \to\quad  \sum_{i = 1}^M f_i(\theta^*) = 0\\
&  \to  \forall i \ge 1: a_i^*f_i(\theta^*) = 0
\end{align}$$
## Solving SVM's primal problem
### Calculate the Lagrangian
- We first calculate the Lagrangian
$$L(\w, b, \mathbf {\alpha}) = \frac 12 \w^T\w - \sum_{i = 1}^N \alpha_i[y_i(\w^T\x_i + b) - 1]$$
### Minimize the Lagrangian
- Then, we minimize $L(\w, b, \alpha)$ with regards to $\w$ and $b$: 
$$\nabla_w L(\w, b, \alpha) = \w - \sum_{i =1 }^N \alpha_iy_i\x_i \stackrel !=0$$
$$\frac{\partial L}{\partial b} = \sum_{i = 1}^N\alpha_iy_i\stackrel != 0$$
- The weights are a linear conbination of the training samples: 
$$\w = \sum_{i =1}^N\alpha_iy_i\x_i $$
### Minimize the $g(\alpha)$
- This term is plugged into the $g(\alpha)$:
$$g(\alpha) = \sum_{i = 1}^N \alpha_i - \frac 12 \sum_{i = 1}^N \sum_{j = 1}^{N}y_iy_j\alpha_i\alpha_j\x_i^T\x_j$$
- $g(\alpha)$ is maximized, subject to the following constraints: 
$$\sum_{i = 1}^N\alpha_iy_i = 0$$
$$\alpha_i \ge 0, \quad \text{for } i = 1,...,N$$
- In vector form, $g(\alpha)$ can be written like so: 
$$g(\alpha)  = \frac12 \alpha^T\Q \alpha + \alpha \I_N$$
- $\Q$ is negative semi-definite and the constraints on $\alpha$ are linear
- This is a **quadratic programming** problem, so an efficient algorithm returns $\alpha^*$
### Recovering $\w$ and $b$ from the dual solution $\alpha^*$
- Having found $\alpha^*$, recall that the weights are a linear combination of the training samples: 
$$\w = \sum_{i = 1}^N\alpha_i^*y_i\x_i$$
- From the complementary slackness condition, $\alpha_i^*f_i(\theta^*) = 0$, we knot what for any $\x_i$ where $\color{lightgreen}{\alpha_i \neq 0}$, the constraint $f_i(\w, b)$ must be $0$: 
$$y_o(\w^T \x_i + b) = 1$$
- Solving for $b$, yields the bias: 
$$b = \frac {1}{\underbrace{y_i}_{\in \{-1, 1\}}} - \w^T\x_i = y_i - \w^T\x_i$$
### Support vectors
- The training samples $\x_i$ with $\alpha_i \neq 0$ are called **support vectors**
- These are the samples on the edge of the margin
![[support vectors.png|250]]
- We define: 
$$\mathcal S = \{i \mid \alpha_i \neq 0\}$$
### Classification
- The class of $\x$ is given by:
$$h(\x) = \text{sign}(\w^T\x + b)$$
- By substituting:
$$\w = \sum_{i = 1}^N \alpha_iy_i\x_i = \sum_{i \in \mathcal S} \alpha_iy_i\x_i$$
- We recieve: 
$$h(\x) = \text{sign}\left(\sum_{i \in \mathcal S}\alpha_iy_i\x_i^T\x + b\right)$$
# Soft Margin Support Vector Machines 
- When classes are not linearly separable due to noise, the Lagrange Multiplier $\alpha^*$ goes to infinity
- To make the model more robust in the presence of noise, constraints are **relaxed**, but the relaxation of a constraint is punished
- For each training sample $\x_i$, a **slack variable** $\xi_i \ge 0$ is introduced, that specifies how far the margin is violated
- The relaxed constraints are:
$$\w^T\x_i + b \ge 1 - \varepsilon_i \quad \text{ for } y_i = 1$$
$$\w^T\x_i + b \ge - 1 + \varepsilon_i\quad \text{ for }y_i = -1$$
$$y_i(\w^T\x_i+b)\ge 1-\xi_i \quad \forall i$$

- The updated cost function is
$$f_0(\w, b, \xi) = \frac 12\w^T\w+C \sum_{i = 1}^{N}\xi_i$$
- $C > 0$ determines how strong a violation is punished
- $C \to \infty$ is the same as hard margin SVMs
![[soft margin svm.png|300]]
## Optimization Problem with Slack Variables
- To find the hyperplane that separates most of the data with maximum margin, we minimize:
$$ f_0(\w, b, \xi) = \frac 12 \w^T\w+ C\sum_{i = 1}^N\xi_i$$
- Subject to: 
$$ \begin{align}
y_i (\w^T\x_i + b)-1 + \xi_i \ge 0 & \quad i = 1,...,N\\
\xi_i \ge 0 &\quad i = 1,...,N
\end{align}$$

### Calculating the Lagrangian
$$L(\w, b, \xi, \alpha, \mu) = \frac 12 \w^T\w+ C\sum_{i = 1}^N\xi_i - \sum_{i = 1}^N\alpha_i[y_i(\w^T\x_i + b) -1 + \xi_i] -\sum_{i = 1}^{N}\mu_i\xi_i$$ 
### Minimize the $L(\w, b, \xi, \alpha, \mu)$ with regards to $\w, b$ and $\xi$
$$\nabla_wL(\w, b, \xi, \alpha, \mu) = \w - \sum_{i = 1}^N\alpha_iy_i\x_i \stackrel != 0$$
$$\frac{\partial L}{\partial \xi_i} = \sum_{i = 1}^{N}\alpha_i\xi_i \stackrel != 0$$
$$\frac{\partial L}{\partial \xi_i} = C -\alpha_i-\mu_i \stackrel != 0 \quad \text{ for } i = 1,...,N$$
- From $\alpha_i = C-\mu_i$ and dual feasiability  $\mu_i \ge 0, \alpha_i \ge 0$ we get $0 \le \alpha_i \le C$
- We arrive at the dual problem with slack variables: 
$$ \begin{align}
&\text{maximize}\quad k \quad g(\alpha) = \sum_{i = 1}^{N}\alpha_i- \frac 12 \sum_{i = 1}^{N}\sum_{j = 1}^{N}y_iy_j\alpha_i\alpha_j\x_i^T\x_j\\ 
&\text{subject to} \quad \sum_{i = 1}^N\alpha_iy_i = 0\\
& \qquad \qquad \quad \;\; 0 \le \alpha_i \le C \quad \text{ for } i = 1,...,N 
\end{align} $$
- The only change from the hard margin dual problem is the constraint $\alpha_i \le C$
## Hinge Loss Fromulation
- A different formulation of the same problem follows: 
$$\begin{align}
\text{minimize} \quad & f_0(\w, b, \xi) = \frac 12 \w^T\w+ C\sum_{i = 1}^N\xi_i \\
\text{subject to } \quad &y_i (\w^T\x_i + b)-1 + \xi_i \ge 0 \quad  i = 1,...,N\\
&\xi_i \ge 0 \qquad \qquad  \qquad \quad \;\, \qquad i = 1,...,N
\end{align}$$
- The optimal solution for the slack variables is: 
$$\xi_i = \begin{cases}
1-y_i(\w^T\x_i+b)\quad &\text{if }y_i(\w^Tx_i+b)<1\\
0 \quad &\text{else}
\end{cases}$$
- The objective function can be rewritten as the **hinge loss formulation**:
$$\min_{\w, b}\frac12 \w^T\w+C\sum_{i = 1}^{N}\max\left\{0, 1-y_i(\w^T\x_i+b)\right\}$$
- The **hinge loss function** penalizes points within the margin: 
$$E_{hinge}(z) = \max\{0, 1-z\}$$


## Classifying a point
- We again denote the set of indices of support vectors as $\mathcal S$, and can write 
$$\w = \sum_{i \in \mathcal S}\alpha_i y_i \x_i$$
- If $0 < \alpha_i < C$, then $\xi_i = 0$, and if $\alpha_i = C$, $\xi_i > 0$
- The bias $b$ can be recovered the same way as in regular SVMs:
$$b = \frac {1}{y_i} - \w^T\x_i = y_i - \sum_{j \in \mathcal S}\alpha_jy_j\x_i^T\x_j$$
- A new point can be classified as:
$$h(\x) = \text{sign}\left(\sum_{j \in \mathcal S}\alpha_jy_j\x_j^T\x + b\right)$$
# Kernels
- To make non-linearly separable functions linearly separabe, basis functions were applied to the input vectors
- In the dual formulation of SVM, the $\x_i$ only enter the dual objective as inner products: 
$$g(\alpha) = \sum_{i = 1}^{N}\alpha_i- \frac 12 \sum_{i = 1}^{N}\sum_{j = 1}^{N}y_iy_j\alpha_i\alpha_j\x_i^T\x_j$$
- This means the basis function is applied like so: 
$$g(\alpha) = \sum_{i = 1}^{N}\alpha_i- \frac 12 \sum_{i = 1}^{N}\sum_{j = 1}^{N}y_iy_j\alpha_i\alpha_j\phi(\x_i)^T\phi(\x_j)$$
- That means a **kernel function** $k: \mathbb R^D \times \mathbb R^D\to \mathbb R$ can be defined: 
$$k(\x_i, \x_j) := \phi(\x_i)^T\phi(x_j)$$
- The dual can be rewritten as: 
$$g(\alpha) = \sum_{i = 1}^{N}\alpha_i- \frac 12 \sum_{i = 1}^{N}\sum_{j = 1}^{N}y_iy_j\alpha_i\alpha_jk(\x_i, \x_j)$$
- The bias can be recovered like so: 
$$b = y_i -\left( \sum_{j \in \mathcal S} \alpha_jy_jk(\x_i, \x_j)\right)$$
- A new point is classified like so: 
$$h(\x) = \text{sign}\left(\sum_{j \in \mathcal S}\alpha_jy_jk(\x_j, \x)+ b  \right)$$
## Advantages
- Oftentimes evaluating a kernel function is easier than evaluating a (possibly infinite-dimensional) basis function
- Kernels can be defined to encode similarity between arbitrary non-numerical data, such as
$$k(\text{apple}, \text{orange}) = -5 \qquad k(\text{lemon}, \text{lime}) = 10$$
## Validity of a Kernel
- A kernel is valid if it corresponds to an inner product in some feature space
- **Mercer's theorem** phrases is like so: 
- A kernel is valid if it gives rise to a **symmetric, positive semidefinite** kernel matrix $\K$ for any input data $\X$.
- The kernel/gram matrix is defined like so:
$$\K  = \begin{pmatrix}
k(\x_1, \x_1) & k(\x_1, \x_2) &... &k(\x_1, x_n)\\
k(\x_2, \x_1) & k(\x_2, \x_2) &... &k(\x_2, x_n)\\
\vdots & \vdots & \ddots & \vdots \\
k(\x_N, \x_1) & k(\x_N, \x_2) &... &k(\x_N, x_n)\\
\end{pmatrix}$$
- If a non-valid kernel is used, the optimization problem may end up being non-convex
## Kernel Preserving Operations
- The kernels $k_1: \upchi \times \upchi \to \mathbb R$ and $k_2: \upchi \times \upchi \to \mathbb R$ with $\upchi \subseteq \mathbb R^N$ are preserved under the following operations: 
$$ k(\x_1, \x_2) = k_1(x_1, x_2) + k_2(\x_1, \x_2) $$
$$k(\x_1, \x_2) = c \cdot k_1(\x_1, \x_2) \\ $$
$$ k(\x_1, \x_2) = k_1(\x_1, \x_2) \cdot k_2(\x_1, \x_2) $$
- Let $k_3$ be defined on $\upchi' \subseteq \mathbb R^M$ and $\phi: \upchi \to \upchi'$
$$k(\x_1, \x_2) = k_3(\phi(\x_1), \phi(\x_2))$$
- Let $\A \in \mathbb R^N \times \mathbb R^N$ be symmetric and positive definite: 
$$k(\x_1, \x_2) = \x^T_1\A\x_2$$
# Remarks
## Hyperparameter Choice
- The best settings for $C$ and the kernel hyperparameters can be determined by performing **cross validation** with **random search** over the parameter space![[svm & kernel hyperparam choice.png|300]]
## Multiple Classes
- The SVM model that was introduced here cannot handle multiple classes
- In the **One-vs-rest** approach, $C$ SVM models are trained for $C$ classes and a data point is classified as the class where the distance from the hyperplane is maximal
- In **One-vs-one**, $\binom{C}{2}$ SVM models are trained and the winning class is the one with the majority vote, where votes are weighted by the distance from the margin 