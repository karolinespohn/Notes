# Maximum Likelihood Estimates
- To estimate underlying probabilities of a probabilistic phenomenon, one can use maximum likelihood estimators
## Example
$$\text{H T H H T H H H T H }$$
- Let $\theta_i$ be the probability that the $i$-th coinflip comes up tails
- And aim to find $\theta$ such that the following term is maximized:
$$p(\text{H T H H T H H H T H }\mid \theta) = \prod_{i = 1}^{10}p(F_i = f_i \mid \theta) = \theta^3(1-\theta)^7$$
### Approximating $\theta$
- To derive $\theta$, the likelihood function must be maximized
- Therefore, the derivative is set to $0$ and solved
- For complicated likelihood functions, the logarithm of the function can be used
## Limitations
- Oftentimes, simply using a maximum likelihood estimate does not accurately reflect reality
- For instance, if you flip a coin twice, and it comes out head both times, you would assign $\Pr[\text{coin comes out tails}]= 0$, which we empirically know to be false
# Bayesan Inference
- A **prior distribution** $p(\theta)$ reflects our belief about theta 
- $p(\theta)$ must not depend on data, be $\ge 0$ for all $\theta$ and $\int p(\theta)\mathrm d\theta = 1$ 
![[Prior Distribution.png|400]]
- Prior $1$ is **non informative**, so it reflects no bias
- Priors $2$ and $4$ reflect a slight and strong belief  that  $\theta = 0, 5$ respectively and prior $3$ reflects a belief that $\theta < 0,5$
- The **posterior distribution** $p(\theta|\mathcal D)$ is derived through **Bayes** and it encodes our beliefs in the value of $\theta$

$$p(\theta|\mathcal D) = \frac{p(\mathcal D|\theta) \cdot p(\theta)}{p(\mathcal D)}$$
- $p(\mathcal D\mid \theta)$ is the **likelihood** and $p(\mathcal D)$ is the evidence, serving as a normalizing constant, and $p(\theta)$ is the prior
- $p(\mathcal D)$ can be derived using the likelihood and the prior: 
$$p(\mathcal D) = \int p(\mathcal D \mid \theta) p(\theta)\mathrm d\theta $$
- It holds that: 
$$\text{posterior $\propto$ likelihood $\cdot$ prior}$$
![[from prior to posterior.png|400]] 
# Maximum a posteriori estimation
- In MAP, $\theta$ is chosen to maximize the likelihood of the posterior distribution
$$\theta_{\text{MAP}} = \arg \max_{\theta} p(\theta \mid \mathcal D) = \arg\max_{\theta}\frac{p(\mathcal D\mid \theta) \cdot p(\theta)}{p(\mathcal D)} = \arg \max_{\theta} p(\mathcal D \mid \theta) \cdot p(\theta)$$
## Approximating $\theta_{\text{MAP}}$ 
- We established the likelihood function for the coin toss example to look like this:
$$p(\mathcal D\mid \theta) = \theta^{|T|} \cdot ( 1 - \theta)^{|H|} $$
- We choose a $\beta$-distribution as a prior
$$\text{Beta}(\theta \mid a,b) = p(\theta \mid a, b) = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)}\theta^{a-1}(1-\theta)^{b-1}$$
- Since $\text{posterior} \propto \text{likelihood} \cdot \text{prior}$, and  $\frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)}$ the following also holds:
$$\begin{align}
p(\theta\mid \mathcal D )&\propto \theta^{|T|}(1-\theta)^{|H|} \cdot \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)}\theta^{a-1}(1-\theta)^{b-1}\\
& \propto \theta^{|T| + a -1}(1-\theta)^{|H| + b -1}
\end{align}$$

- The closed form solution of   goes as follows: 
- We want the solution of 
$$\begin{align}
\theta_{\text{MAP}} &= \arg \max_{\theta}p(\theta\mid \mathcal D) \\ 
&= \arg \max_{\theta} \theta^{|T| + a - 1}(1-\theta)^{|H| + b -1} \\
&= \arg \max_{\theta} (|T| + a -1)\log\theta + (|H| + b - 1)\log(1 - \theta)
\end{align}$$
- And we arrive at
$$\theta_{\text{MAP}} = \frac{|T| + a - 1}{|H| + |T| + a + b -2}$$
# Estimating posterior distribution
- Using MAP, we recieve the most probable value for $\theta$ under the posterior distribution, but to determine $\mathbb E[\theta]$ or $\text{Var}[\theta]$, we need to determine the posterior distribution
- We know the posterior up to a normalization constant, such that $p(\theta \mid \mathcal D)$ integrates to $1$
$$p(\theta \mid \mathcal D) \propto \theta^{|T| + a -1}(1-\theta)^{|H| + b -1}$$
## Finding the normalization constant
### Brute Force
- One way of finding the normalization constant is solving the following: 
  $$\int_{0}^{1} \theta^{|T| + a -1}(1-\theta)^{|H|+b-1}\mathrm d \theta \stackrel{!}{=}1$$
### Pattern Matching
- The unnormalized posterior is similar to the PDF of the Beta distribution
$$p(\theta \mid \mathcal D) \propto \theta^{|T| + a -1}(1-\theta)^{|H| + b -1}$$
$$\text{Beta}(\theta \mid \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\alpha -1}(1-\theta)^{\beta -1}$$
- We conclude that the normalizing constant is:
$$\frac{\Gamma(|T| + a + |H| + b)}{\Gamma(|T| + a)\Gamma(|H| + b)}$$
- The posterior is a Beta distribution: 
$$p(\theta| D) = \text{Beta}(\theta \mid a + |T |, b + |H|)$$
#### Conjugate Priors
- Posterior and prior are in the same family of distribution because the distribution of the prior is a conjugate prior of the likelihood
- Specifically, the $\beta$-distribition of the prior is a conjugate prior to the Bernoulli distribution of the likelihood, resulting in a $\beta$ distributed posterior
# Predicting the next toss
## MLE
- To predict the next coin toss using MLE, we compute $\theta_{\text{MLE}}$ predict that
$$p(F_{n + 1}= \mathrm T \mid \theta_{\text{MLE}}) = \text{Ber}(F_{n + 1} =\mathrm T \mid \theta_{\text{MLE}}) = \theta_{\text{MLE}}$$
## MAP
- To predict the next coin toss using MAP, we compute $\theta_{\text{MAP}}$ predict that
$$p(F_{n + 1}= \mathrm T \mid \theta_{\text{MAP}}) = \text{Ber}(F_{n + 1} =\mathrm T \mid \theta_{\text{MAP}}) = \theta_{\text{MAP}}$$
## Posterior distribution (Fully Bayesian)
- If we have the posterior distribution $p(\theta \mid \mathcal D, a, b)$ of $\theta$, then to predict the next coin flip, we want to estimate the **posterior predictive** distribution
$$p(F = \text{T}\mid \mathcal D, a, b)$$
- The posterior predictive distribution is about predicting future data using the parameter $\theta$, whereas the posterior distribution is about estimating $\theta$
### Example
- We denote the outcome of the next flip as $f \in \{0, 1\}$
$$p(F = f \mid \mathcal D, a, b) = p(f\mid \mathcal D, a, b) = \int_0^1p(f\mid \theta) p(\theta \mid \mathcal D, a, b) \mathrm d \theta$$
// TODO