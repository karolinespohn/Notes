$$\DeclareMathOperator{\Pr}{\mathbb{P}}\DeclareMathOperator{\Var}{\text{Var}}\DeclareMathOperator{\Cov}{\text{Cov}}\DeclareMathOperator{\E}{\mathbb E}\DeclareMathOperator{\Corr}{\text{Corr}}\DeclareMathOperator{\RSE}{\text{RSE}}\DeclareMathOperator{\RSS}{\text{RSS}}\DeclareMathOperator{\TSS}{\text{TSS}}$$
# Definitions
- The goal of inference is finding estimates for unknown parameters of distributions by looking at samples & computing statistics
## Measures of Error
- **Bias** refers to the tendency for a statistic $T(X)$ to over- or underestimate a true parameter $\theta$
$$\mathbb E[T(X)]-\theta$$
- The **standard error** of $T(X)$ is denoted by: 
$$\text{SE}(T(X)) = \sqrt{\text{Var}(T(X))}$$
- The **sample mean** and **sample variance** are: 
$$\overline{X} = \frac{1}{n} \sum_{i = 1}^n X_i, \; \; S^2 = \frac{1}{n - 1}\sum_{i = 1}^n(X_i - \overline{X})^2$$
## Sampling distribution
- Given a sample $X_1,...,X_n$ and a statistic $T(X)$, we call the distribution of $T(X)$ **sampling distribution** 
- To compensate for the noise in a sample, there should ideally be multiple samples taken
- However, oftentimes, this is not possible, and can be compensated for using different approaches
### Theoretical Approach
- In the theoretical approach, the assumption that the sample $X$ follows a certain distribution is made
- The distribution of the sample $T(X)$ can therefore be infered
### Asymptotic Approach
- The **Central Limit Theorem** states that for a series of iid random variables $X_1,...,X_n$, with the mean $\mu$ and variance $\sigma^2$, as $n$ goes to $\infty$, the sample mean $\overline X=\frac 1n \sum_{i =1}^n X_i$ will follow a normal distribution with mean $\mu$ and variance $\frac{\sigma^2}n$
$$\overline X \sim \mathcal N\left(\mu \frac{\sigma^2}n\right)$$
- The asymptomatic approach uses this to determine the actual mean $\mu$ and variance $\sigma^2$ of the underlying distribution of $X_1, ..., X_n$ 
### Bootstrap Approach
- In the bootstrap approach, given a sample $X_1, ..., X_n$ and a number of resamples $B$, the process of obtaining new samples is simulated in the following way: 
	- $n$ random observations are selected from $X$ with resampling $B$ times, creating a resample: 
$$X^b = \{X_1^b, ..., X_b^n\}$$
	- For each resasmple, $T(X^b)$ is computed
	-  The $T(X^b)$ statistics are used to infer information about the sampling distribution

# Confidence Intervals
- Confidence intervalls allow for the specification of not only a point estimate but also a measure of uncertainty
## Theoretical Approach
- If the underlying distribution is assumed with a high probability, properties of the statistics can be used to infer the confidence interval
## Asymptotic Approach
- If the central limit theorem is applicable to the sampling distribution of a point estimator $T(X)$, a confidence interval for $1-\alpha$ can be constructed as
$$T(X) \pm z_{1 - \frac\alpha2} \cdot \text{SE}(T(X))$$
- $z_{1-\frac\alpha2}$ denotes the $1 - \frac \alpha2$ quantile of the $\mathcal N(0, 1)$ distribution
- This implies: 
$$P(x < z_{1 - \alpha/2}) \leq 1 - \alpha/2$$
## Bootstrap Approach
- Different methods can be used to construct a confidence interval for a bootstrap estimate of a parameter $\theta$
### Percentile Method
- Every element in a bootstrap distribution $T(X^b$) is an estimator for the population parameter $\theta$
- A $(1-\alpha)$ estimate can be computed by choosing boundaries that include the middle $(1-\alpha)$ elements
- The $\frac \alpha2$ percentile is the left boundary and the $1-\frac \alpha2$ percentile is the right boundary
### Standard Error Method
- For symmetric bootstrap distributions, the same formula as in the asymptotic approach can be used
$$T(X) \pm z_{1 - \frac\alpha2} \cdot \text{SE}(T(X))$$
# Hypothesis Testing
- A **null-hypothesis** $H_0$ is formed, stating that there is no underlying structure in the observations and they are essentially random - no correlations can be found
- The alternative hypothesis represents the alternative to this claim
## Statistical Tests
- A statistical test decides, whether $H_0$ can be accepted or rejected based on underlying data $X_0, ..., X_n$
- A **test statistic** $T(X)$ is chosen, and the range of $T(X)$ is split into a **rejection region** $R$ and its complement
- $H_0$ is rejected if $T(X) \in R$
### Errors
- **Type 1 Errors** occur if $H_0$ is rejected despite being true, and they occur with the probability $\le\alpha$
- **Type 2 Errors** occur if $H_0$ is not rejected despite being false
### P-Value
- The $p$-value quantifies the strength of the data in support of/against $H_0$
- If the $p$-value is lower than a predefined threshold, the hypothesis is rejected
#### One- and Two-Sided Alternatives
- **One-Sided**: 
$$H_0: \theta = \theta_0 \qquad H_A: \theta\neq\theta_0$$
- **Two-Sided**:
$$H_0: \theta = \theta_0 \qquad H_A: \theta <\theta_0$$
$$H_0: \theta = \theta_0 \qquad H_A: \theta >\theta_0$$
### Tests
#### Bernoulli Test
- To test if a random variable is Bernoulli distributed, with a success probability $q$, binomial tests can be created in R like so: 
```
binomial.test(x = num_success, n = num_total, p = q, alternative = "two.sided")
```
#### T-Test
- To test if the mean of a normally distributed random variable is above or below some value $\theta$, a t test can be used: 
```
t.test(data_col, mu = theta, alternative="less")
```
#### Power of a test
- The power of a test describes the probability of correctly rejecting $H_0$, $1-\beta$
```
power.t.test(
	delta = <effect size>, 
	sd = <sd>, 
	sig.level = 0.05,
	power = 0.8,
	type = "one.sample",
	alternative = "one.sided")
```