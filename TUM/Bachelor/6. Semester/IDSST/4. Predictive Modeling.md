$$\DeclareMathOperator{\Pr}{\mathbb{P}}\DeclareMathOperator{\Var}{\text{Var}}\DeclareMathOperator{\Cov}{\text{Cov}}\DeclareMathOperator{\E}{\mathbb E}\DeclareMathOperator{\Corr}{\text{Corr}}\DeclareMathOperator{\RSE}{\text{RSE}}\DeclareMathOperator{\RSS}{\text{RSS}}\DeclareMathOperator{\TSS}{\text{TSS}}$$
# Introduction
- In supervised learning, a training sample $X$ has a corresponding target $Y$, and the goal is finding a function, that predicts the target $y$ for any input $x$
- If the output is numerical, this is a **regression problem**, if the output is categorical, it is a **classification problem**
- Noisy relationships can be modelled with a function $f_0$ and an error term $\epsilon_0$
$$Y=f_0(X) + \epsilon_0$$ 
# Models
- The unknown $f_0$ can be approximated with a linear function
$$f(x) = \beta_0 + \beta_1 x$$
- The parameters $\beta_0$ and $\beta_1$ are chosen to minimize the error
- For linear functions this can for instance be done through **linear regression**
# Model Quality
- The most common measurement for model quality is **mean squared error**
$$\text{MSE} = \frac 1n\sum_{i = 1}^n(y_i - \hat f(x_i))^2$$
- The **training MSE** is the MSE on the train set whereas the **test MSE** is the MSE on the test set
## Bias-Variance Tradeoff
- Bias is the expected error due to model mismatch; rigid models, misspecifications,... lead to high bias
- Variance refers to variation due to randomnes in the training data, oftentimes because it the model memorized the data 
- Formally, the error of a model can be written as: 
$$E\left[\left(Y^*-\hat f(x^*)\right)^2\right] = \Var\left[\hat f(x^*)\right] + \left[\text{Bias}\left(\hat f(x^*)\right)\right]^2 + \Var[\epsilon_0]$$
$$\text{Bias}\left(\hat f(x^*)\right) = E\left[\hat f (x^*) \right] - f_0(x^*)$$
### Example
- In the following graphic, the test MSE is gray and the training MSE is red
- The U-shape can be explained like so:
	- In the beginning, the bias is high and therefore the MSE is also high - the model doesn't capture the relationships between $X$ and $Y$ 
	- Then, the bias goes down, and the variance is still low
	- After a while, the variance starts to increase, as the model starts to memorize data instead of providing a general solution
![[Screenshot 2025-06-08 at 14.38.05.png|300]]
## $k$-Fold cross validation
- To improve the model quality, **hyperparameters** can be tuned
- This can be done through $k$-fold cross validation: 
	- The data is split into $k$ parts
	- The model is trained several different times, leaving each of the $k$ parts out once and using it as a validation set
	- The hyperparameters with the lowest MSE are used for the model
![[hp tuning.png]]
# Linear Regression
- Linear regression is used for functions with a persumed linear relationship between the predictor variable $X$ and the output $Y$
$$Y \approx \beta_0 + \beta_1 X$$
- In $R$, a linear regression model is fitted with the `lm` function
## Least Squares
- Since the regression paramaters $\beta_0$ and $\beta_1$ are unknown, they can be approximated with **least squares**
- The **residual sum of squares** must be minimized to find the optimal parameters $\hat \beta_0$ and $\hat \beta_1$
$$
(\hat \beta_{0,n}, \hat \beta_{1,n}) =\text{argmin}_{(b_0, b_1)\in \mathbb R^2}\;\RSS(b_0, b_1) 
= \sum_{i = 1}^ne_i^2 = \sum_{i = 1}^n(y_i - (b_0 + b_1x_i))^2
$$
- The estimates for $\beta_{0,n}$ and $\beta_{i,n}$ are given by: 
$$\hat \beta_{1,n}= \frac{\sum_{i = 1}^n(x_i - \overline x_n)(y_i - \overline y_n)}{\sum_{i = 1}^n(x_i - \overline x_n)^2}$$
$$\hat \beta_{0, n} =  \overline y_n - \hat \beta_1\overline x_n$$
### Closed Form Solution
- The closed form solution is: 
$$\hat{\beta} = (X^TX)^{-1}X^Ty$$
### Predictions
- Knowing the parameters, a prediction for a data set $X$ can be made like so: 
$$y \approx X\hat b= \begin{pmatrix}
1 & x_1\\ 1& x_2 \\ \vdots & \vdots \\ 1 & x_n
\end{pmatrix}$$
## Model Quality
- The quality of a linear regression fit can be evaluated through vatious methods
### RSE
- The **residual standard error** measured the standard deviation of $\epsilon_0$ under the assumption $y = \beta_0 + \beta_1 \cdot X + \epsilon_0$
$$\RSE= \sqrt{\frac{1}{n-2}\RSS} = \sqrt{\frac{1}{n-2}\sum_{i = 1}^n(y_i-\hat y_i)^2}$$
- In R, the standard error is denoted as `sigma` and it can be calculated through the `glance` function
### $R^2$
- Since the $\RSE$ is measured in units of the response variable, it can be unclear whether it is good or not
- The $R^2$ ($R$ squared value) measures the model quality proportionately to the total sum of squares ($\TSS$)
$$R^2 = \frac {\TSS-\RSS}{\TSS}$$
- $R^2$ describes the percentage of variance in the described model that can be explained by the model 
- If $R^2 = 1$, that would mean that the model can perfectly explain why the variance in the data is the way it is
### Residual Plot
- To analyze the fit of a model a residual plot can be created in R
- Here, the regression line is drawn on top of the datapoints
- If a clear structure is visible, the linear  model might not fit correctly
![[residual plot.png|250]]
# Multipe Linear Regression
- Oftentimes, multiple explanatory variables are necessary to explain a response variable
- A simple linear regression model can be expanded by giving each explanatory variable $x_{i,j}$ a weight $\beta_j$
$$Y_i = \beta_0 + \beta_1 x_{i, 1} + ... + \beta_k x_{i, k} + \epsilon_i$$
## Weight Calculations
- The ideal weights $\hat \beta$ can be estimated using least squares: 
$$\hat{\beta} = \arg \min_{b}RSS(b) = \sum_{i = 1}^n ((b_0 + b_1 x_{i, 1} + ... + b_k x_{i, k}) - y_i)^2$$
$$\hat{\beta} = (X^TX)^{-1}X^Ty$$
- Least squares estimates can be computed like so: 
$$\hat{y} = X \hat{\beta} = X (X^TX)^{-1}X^Ty = Hy$$
- $H$ is called the **hat matrix**
- The variance can be calculated as:
$$\hat \sigma^2 = \frac{\Sigma_{i = 1}^ne_i^2}{n-k-1}$$
## Model Quality
- $R^2$ naturally increases as the number of variable increases, making the estimate of the percentage of variablity biased
- It must therefore be adjusted like so: 
$$R_{adj}^2 = 1 - \frac{\sum_{i = 1}^n (y_i - \hat{y}_i)^2}{\sum_{i = 1}^n (y_i - \overline{y}_n)^2} \cdot \frac{n - 1}{n - k - 1} = R^2 \cdot \frac{n - 1}{n - k - 1}$$
## Collinearity between Predictors
- If different explanatory variables corellate with one another, this is called **Multicollinearity**
- Two variables that are not independent are called **collinear**
- In that case, the weights for the linear regression model become difficult to interpret
# Logistic Regression
- A hyperplane is defined, splitting the data into two categories
- A new data point is classified by determining on which side of the hyperplane it is
- To find $P(y_i = 1 \mid x_i)$, we use the **inverse logit function**
$$p(y_i = 1 \mid x_i) = \sigma(\mu_i) = \frac{1}{1 + \exp(-\mu_i)}$$
- In R, a logistic regression model is created with the `glm` function with `family` set to `binomial`
## Odds
- In logistic regression, oftentimes the odds are considered, i. e.: 
$$\text{odds}(\{Y_i = 1\} \mid x) = \frac{Pr(\{Y_i = 1\} \mid x)}{1 - Pr(\{Y_i = 1\} \mid x)}$$
- To interpret the results from a logistic regression model, the **odds ratio** is often used
$$OR = \frac{\text{odds when } x_{i, j} = x_{i, j} + 1}{\text{odds when } x_{i, j} = x_{i, j}} = \frac{e^{\beta_0} \cdot e^{\beta_1 x_{i, 1}} \cdot ... \cdot e^{\beta_j (x_{i, j} + 1)} \cdot ...}{e^{\beta_0} \cdot e^{\beta_1 x_{i, 1}} \cdot ... \cdot e^{\beta_j x_{i, j}} \cdot ...} = e^{\beta_j}$$
- Example: 
	- Let `var1` be a variable with the slope $-0.00345$ 
	- If we increase `var1` by 10, we get
$$e^{-0.00345 \cdot 10}\approx 0.97$$
	- This means, the odds of an observation being classified as belonging to class $y_i$ are decreased by $3\%$ if `var1` is increased by 10
### Relative Risk
- The odds ratio is not equivalent to the relative risk
- In R, to obtain the relative risk, the `oddsratio_to_riskratio` function can be used