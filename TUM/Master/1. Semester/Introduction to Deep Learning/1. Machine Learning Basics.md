- ML is often about fitting functions to data
- There are different ways to do that, including knn, regression, ...
# Linear Models
- Linear models can be expressed as $\hat y_i = \theta_0 +  \sum_{j = 1}^d x_{ij}\theta_j$
- It can be expressed in matrix form: 
$$
\begin{pmatrix}
\hat y_1 \\
\hat y_2 \\
\vdots \\
\hat y_n
\end{pmatrix}
= 
\begin{pmatrix}
1 & x_{11} &... & x_{1d}\\
1 & x_{21} & ... & x_{2d}\\
\vdots & \vdots & \ddots & \vdots \\
1& x_{n1} & ... & x_{nd}
\end{pmatrix}
\begin{pmatrix}
\theta_0\\
\theta_1\\
\vdots 
\theta_d
\end{pmatrix}
$$
$$\hat y = X  \theta$$
- The closed form solution for $\theta$ is 
$$\theta = (X^TX)^{-1}X^T$$
## Least Squares
- For linear models, a common way of fitting a linear model to data is the **least squares** approach: 
$$\min_\theta J(\theta) = \frac 1n\sum_{i = 1}^n(\hat y_i - y_i)^2= \frac 1n\sum_{i = 1}^n(x_i \theta - y_i)^2 = (X\theta -y)^T(X\theta -y)$$
- where...
	- ...$n$ is the number of training samples
	- ...$\hat y_i = x_i\theta$ is the estimation from the model
	- ...$y_i$ is the actual label 
## Maximum Likelihood
- In MLE, you consider what the most likely label for data is given the training data and the parameters
$$\theta_{ML} = \arg \max _\theta p_{model}(y\mid X, \theta)$$
- MLE assumes that training samples are indepedent and generated by the same probability distribution
$$p_{model}(y\mid X, \theta) = \prod_{i = 1}^n p_{model}(y_i, \theta) = \prod_{i = 1}^n \log p_{model}(y_i, \theta)$$
## Logistic Regression
- Logistic regression is a generalization of linear regression
- Logistic regression can be applied to **2-class** classification problems
- It predicts the probability for binary outcomes
