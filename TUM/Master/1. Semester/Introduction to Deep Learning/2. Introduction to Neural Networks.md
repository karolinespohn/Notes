- Sometimes, datasets are not **linearly separable**
![[linear separability.png|400]]
- **Non-linearity** can be introduced 
- The goal is to find optimal weights and biases $W$ to minimize a loss function for a given dataset 
# Computational Graphs
- Computational graphs are directional graphs where matrix operations are represented as compute nodes
- During the forward pass, the nodes are evaluated
![[computational graph.png|300]]
- A neural network is can be representeded as a computational graph, since it...
	- ...has compute nodes (operations)
	- ...has edges to connect the nodes (data flow)
	- ...is directional
	- ...can be organized into layers
# Loss Functions
- A **loss function** measures the goodness of the predictions
- Large loss indicates bad predictions
- For **regression**, typical functions are :
	- $L_1$ loss$$L(y, \hat y; \theta) = \frac 1n \sum_i^n ||y_i - \hat y_i||_i$$
	- MSE loss: $$L(y, \hat y; \theta) = \frac 1n \sum_i^n||y_i - \hat y_i||_2^2$$
- For binary classification, **binary cross entropy** is often used
$$L(y, \hat y; \theta) = -\frac 1n \sum_i^n \left[y_i \log \hat y + (1 =y_i) \log(1-\hat y_i)\right]$$
-  It generalized into multi-class classification where it is called **cross entropy**
$$L(y, \hat y; \theta) = -\sum_{i=1}^n \sum_{k=1}^k(y_{ik} \cdot \log(\hat y_{ik})$$
- Neural networks are made better, by propagating gradients from the end to the first layer using the **chain rule** 
= 
