$$\DeclareMathOperator{\Var}{\text{Var}}\DeclareMathOperator{\rank}{\text{rank}}$$
# Basics
- In a linear regression model, the **best linear unbiased estimator** is given by **ordinary least squares**, if the errors...
	- ...have expectation 0
	- ...are uncorrelated 
	- ...have equal variances
- **Unbiased** means that $E\left(\hat \beta_j\right) = \beta_j$ 
- **Best** means that it gives the **lowest variance** of the estimate as compared to other linear unbiased estimators
- **Consistency** means that $\Var(\hat \beta)$ decreases with increasing sample size $n$
- An estimator $\hat \beta$ is **efficient** if it has lower variance than any other estimator $\tilde \beta$: $\Var(\hat \beta) < \Var(\tilde\beta)$
![[unbiased consistent efficient.png|400]]
# OLS Optimality Conditions
- The **OLS** is the best linear unbiased estimator, iff the following conditions hold: 
## Linearity
- Parameters have a linear relationship
- **Outliers** can be detected throught **Cook's Distance**: 
$$D_k = \frac{\sum_i\left(\hat y_i = \hat y_{i(k)}\right)}{ps^2}$$
- Where...
	- ...$p$ is the number of independent variables
	- ...$\hat y_{i(k)}$ is the fitted response value excluding observation $k$
	- ...$s^2 = \frac{e^Te}{n-p}$
## Multicollinearity
- Predictors are not linearly dependent
- To check for linear dependencies of two columns in a matrix, te **rank** can be determined
- The observation matrix $X  \in \mathcal R^{n \times (p+1)}$ has an exact linear relationship if: 
$$\rank(X) < p+1$$
### Checking for Multicollinearity
- Multicollinearity can be checked in 2 ways
#### Corellation Coefficient
- The correlation for each pair of predictor variables can be computed
- Correlations that are large by absolute value indicate issues
- Even if pairwise correlations are small, linear dependence can exist between three or more variables
#### Variance Inflation Factor (VIF)
- Alternatively, the variance inflation factor can be estimated
$$\text{VIF}= \frac{1}{1-R^2_K}$$
- Variables should be removed if $\text{VIF} > 10$
- Multicollinearity can lead to small $t$-valies (non-significance), if they are highlt correlated with the response variable and therefore redundant
- To check for multicollinearity, each pair of predictor variables must be checked

## Homoscedasticity
- The residuals have constant variance and are normally distributed
- **Heteroscedasticity** occurrs when the constant variance condition is violated
### Checking for Heteroscedasticity
- Two methods exist to check for heteroscedasticity 
#### Glejser Test
1. The **original** regression is estimated with OLS and the sample residuals $e_i$ are determined
2. The absolute value of the residuals is regressed on the explanatory variable $X$ using different functional forms: 
	$$|e_i| = \gamma_0 + \gamma_1f(X_i) + v_i$$
	- Where $f(x) \in \{\text{identiy(x)}, sqrt(x), \frac{1}{X}\}$
- From these equations, the one with the highest $R^2$ value and the lowest standard error is chosen to represent the form of the heteroscedasticity
- A $t$ test is performed, and if $\gamma$ is statistically significant, $H_0 = \text{Homoscedasticity}$ can be rejected
#### White Test
- The White test is more complex and general
- It doesn't require choosing a particular $X$ or normally distributed residuals
- **Auxiliary regression** is perfomed on all explanatory variables $X_j$, their squares $X_j^2$ and their cross products
- For two variables, $e^2$ is determined like so: 
$$$e^2 = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3 X_1^2+ \beta_4 X_2^2 + \beta_5 X_1X_2 + v$$
- The test statistic is $nR^2$, where $n$ is the sample size and $R^2$ comes form the auxiliary regression
- The hypothesis "$H_0$: all the variances $\sigma_i^2$ are equal" is rejected if $\chi^2 > \chi_\alpha^2$ or if the $p$ value is low
## No Autocorrelation
 - **No autocorrelation**: The $i$th and $j$th residual terms are not correlated
 - The residuals should show no pattern 
 - Autocorrelation can occur because of the omission of an important variable, a functional misfit or a measurement error
 
 ![[Homoscedasticity.png]]
### Check For Autocorrelation
#### Durbin-Watson
 - To test for first-order autocorrelation, the following **Durbin-Watson** statistic can be used
 $$\text{DW} = \frac{\Sigma_{i=n}^n(e_i-e_{i-1})^2}{\Sigma_{i=1}^ne_i^2}$$
- The output takes results between $2$ and $4$: 
	- If $\text{DW}\in \{1.5-2.5\}$ there is no autocorrelation
	- If $\text{DW}=0$, there is **perfect positive autocorrelation**
	- If $\text{DW} =4$ there is **perfect negative autocorrelation** 
#### Breusch-Godfrey
- The Breusch-Godfrey test is more general, considering **higher-order autoregression schemes**
 - $H_0:$ There is no autocorrelation
 - Test Statistic: $(n-p)R^2$, where $n$ is the sample size, $p$ is the order of the autoregressive scheme and $R^2$ is from the auxiliary regression
 - If $(n-p)R^2$ exceeds the critical value, $H_0$ is rejected
## Exogeneity
- **Exogeneity** is given, if the expected value of the residual vector is $0$
- If this is violated, it is called **endogeneity**
- Endogeneity may occur due to ommitted variables
- This can be overcome by using **Panel Data**: While Cross section data observes many subjects at a single point of time, panel data repeatedly observes the same units over multiple time periods
- Endogeneity can be adressed through the fixed effects model
- It assumes endogeneity andi