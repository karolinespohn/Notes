$$\DeclareMathOperator{\entropy}{\text{entropy}}$$
# Decision Tree Basics
- As compared to regular deep learning, decision trees offer **explainability** and **computational efficiency**
- **Internal nodes** of decision trees represent a **test on an attribute**, **branches** represent an **outcome** of the test and **leaf nodes** represent the **class label (/distribution)**
# Choosing Splitting attributes
- At each node, one attribute is chosen to split training examples into distinct classes as much as possible
- Decision trees can be built **top-down** and pruned  **bottom-up**:
	- **Top-down tree construction** starts with all examples at the root, and partitions the examples recursively by choosing one attribute each time
	- **Bottom-up tree pruning** is removing subrees or branches in a bottom-up manner to improve the estimated accuracy on new cases
- To choose the splitting attribute, a **goodness** function is used
- Typical functions include **information gain**, **information gain ratio** and **gini index**
## Best Attributes
- The "best" attribute can be chosen to minimize the tree or to produce the purest nodes
- **Purity Measures**:
	- Should be $0$ is a node is pure
	- Shoule be maximal when the impurtiy is maximal
	- Should fulfill the multistage property (decisions can be made in multiple steps)
### Informatio Gain
- The goal of information gain is creating the purest possible nodes
 - It is calculated as
 $$\text{Information Gain} = \text{(Information Before Split)} - \text{(Information After Split)}$$
- Information is measured in bits 
- The **entropy** of a probability distribution ("Shannon Entropy" $H$) measures the average level of uncertainty or surprise
$$\entropy(p_1,\dots , p_n) = p_1 \log \left(\frac{1}{p_1}\right) + ...+p_n\log \left(\frac{1}{p_n}\right)$$
- The expected information gain is the information gained by adding a sub-tree and it is defined as
$$\text{gain}(S, a) = \entropy(S)=\sum_{v \in \text{values}(a)}\left(\frac{|S_v|}{|S|}\right)$$
- Where...
	- ...$a$ are all possible values for the attribute $a$
	- ...$S_v = \{s \in S: a (s) = v\}$
- For attributes that can take on many different values, this often leads to overfitting
### Information Gain Ratio
 - The gain ration is a modification of the informatio gain to reduce its bias on high branch attributes
 - It takes number and size of branches into account when choosing an attribute
 - It can be calculated as: 
 $$\text{gain ratio}(S, a) = \frac{\text{gain)(S,a)}}{\text{intrinsic info}(S,a)}$$
 $$\text{intrinsic info}(S, a)=-\sum_{i =2}^k\frac{|S_i|}{|S|}\log_2 \left(\frac{|S_i|}{|S|}\right)$$
#### C4.5
- C 4.5 is a decision tree algorithm 
- It uses the gain ration to select the best attribute for splitting the data
### Gini Index
- Gini Indx can be used for the splitting criterion in **Classification and Regression Tree (CART)** 
- For 2 classes $C_p$ with $p$ elements and $C_n$ with $n$ elements it can be calculated as: 
$$P = \frac{p}{p+n}$$
$$N = \frac{n}{p+n}$$
$$\text{Gini}(S) = 1-P^2-N^2 \in [0, 0.5]$$
- If a dataset $S$ is split into $S_1$ and $S_2$ the Gini index is calculated as
$$\text{Gini}_{\text{split}}= \frac{p_1+n_1}{p+n} \cdot \text{Gini}(S_1) + \frac{p_2+n_2}{p + n}$ \cdot \text{Gini}(S_2)$$


- For $m$ attributes, $n$ instances and a tree whose depth is $\in \mathcal O(\log n)$
- The algorithm for building the tree is $\in  \mathcal O(mn \log n)$
# Numeric Attributes
- Numeric attributes have mant different possible split points
- Oftentimes binary splits (e.g $\text{temp} < 45$) are used
- Infromation gain can be measured for every possible split point and the best one can be chosen
- Numerical attributes can be uset multiple times in a decision tree, while nominal attributes can only be used once
# Missing Values
- During training, different ways for handling missing values exist:
	- Ignoring instances with missing values
	- Ignoring attributes with missing values
	- Treating missing values as nomonal values
	- Estimating missing values
- During classification, there are two modes to deal with missing values: 
	- **Follow the leader**: If no decision can be made at a node because the attribute is missing, the item is sent down the branch with the largest number of instances
	- **Partition the instance**: The instance is conceptually partitioned and sent down all numbers proportional to the number of training instances in each branch
# Overfitting & Pruning
- Noise and outliers are sources of abnormalities
- Trying to fit every abnormality leads to overfitting
- Trees can be **pruned**: Subbranches are removed bottom-up
# Relational Rules
- Rules comparing attributes to constants are called **propositional**
- Rules comparing different attributes are called **relational**
![[shapes.png|300]]
- An example of relational rules is given by the shapes problem
```
if width > height: lying
if height >= height: standing
```