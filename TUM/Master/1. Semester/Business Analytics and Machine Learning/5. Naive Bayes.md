$$\DeclareMathOperator{\Pr}{\mathbb P}$$
# Classification
- Given a datapoints $D= \{x_1, x_2, ..., x_n\}$ and a set of classes $C =\{C_1,C_2,...,C_n\}$, the classification problem is to define a mapping $f:D \to C
# Naive Bayes
- Naive Bayes is a classification strategy that assumes that all attributes are **equally important** and **conditionally independent**
- Even if the assumptions don't hold, naive bayes often works well
- The **prior** $\Pr[e]$ represents the unconditional probability that $e$ is true, i.e
$$\Pr[\text{height} > 1.80]=35 \%$$
- The probability changes given more information, and the **posterior** $\Pr[e\mid h]$ represents the conditional probabilityof $e$
$$\Pr[\text{height}> 1.80|\text{Pro Basketball Player}]= 9
\%$$
## Conditional Independence
- Given two (sets of) random variables, independence holds, if
$$\Pr(A, B) = \Pr(A)\Pr(B)$$
- Conditional independence ($\Pr(A \perp B \mid C)$) holds, if $A$ and $B$ are unrelated after taking into account a 3rd variable
$$\Pr(A, B\mid C) =\Pr(A\mid C) \Pr(B\mid C)$$
$$\Pr(A\mid B, C) = \Pr(A\mid C)$$
## Bayes 
- Bayes' Rule states: 
$$\Pr(h\mid e) = \frac{\Pr(e \mid h) \Pr(h)}{\Pr(e)}$$
### Frequency Tables
- Frequency tables are used to calculate the prior probabilities $\Pr(h)$
![[frequency table.png|400]]
![[likelihood table.png|400]]
## Inference
- Given conditionally independent explanatory attributes $e_1, e_2,...,e_n$ and the outcomes $H$ , the predicted class can be calculated through maximum a posteriori estimation: 
$$ h_\text{MAP} = \arg \max_{h \in {H}}\Pr(h) \cdot \prod\Pr(e_i\mid h) $$
- To get the probabilities, **normalization** can be used
$$\Pr(h \mid e_1,e_2,...,e_n) = \frac{\Pr(e_1, e_2,...,e_n)\Pr(h)}{\Pr(e_1, e_2, ..., e_n)}$$
## Zero-Frequency Problems
- If a specific attribute never occurs with a particular class value in the training data, the calculated likelihood for that combination will be 
- Since all likelihoods are multiplied, a single zero has a large effect
- To avoid this, a constant can be added to the numerator for every attribute value-class combination
## Missing Values
- If a value is for an attribute is missing during training, the instance in not included in the **frequency count** for the attribute value-class combination
- If a value for an attribute is missing during inference, that attribute will be **omitted from the calculation**
## Numeric Attributes
- For numeric attributes, normal or Gaussian probability distribution are usually assumed
- The probability density function is defined by the sample mean $\mu = \frac 1n\sum_{i=1}^nx_i$  and the standard deviation $\sigma = \sqrt{\frac{1}{n-1}\sum_{i = 1}^n (x_i - \mu )^2}$ 
- The density function is given by:
$$f(x) = \frac1{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$
### Kernel Density Estimation (KDE)
- If the distribution of a numeric attribute does not follow a normal or Gaussian distribution, **Kernel Density Estimation** is used to estimate the density distribution
- The requirements for the function $f$ are: 
	1. $f(x)$ must be a probability density function: 
$$\int f(x) dx  = 1$$
	2. $f(x)$ is a smooth approximation of the data points in $X$
	3. $f(x)$ can be used to estimate values $x^*$ that are not in the training set
- The **Rosenblatt-Parzen Kernel-Density-Estimator** is given by: 
$$f(x) = \frac{1}{n}\sum_{i = 1}^{n}K(x-x_i, h) $$
	- Where $K(t,h)$ is the kernel function (often Gaussian) and $h$ is the bandwidth which is adjusted to the data
## Discussion
- Naive Bayes works well, even when different attributes are not equally important
- Adding too many redundant attributes leads to issues
- Calculating conditional probabilities is in $\mathcal O(n)$ where $n$ is the number of instances
- Calculating the class is in $\mathcal O(cp)$ where $c$ is the number of classes and $p$ the number of attributes
# Bayesian Belief Networks
- If the assumption of conditional independence is too restrictive, Bayesian (Belief) Networks can be used
- The goal is **describing conditional independence** among subsets of attribute
- For that, **directed acyclic graphs** are used
	- Nodes represent attributes
	- Edges represent direct influence or dependence ($x \to y$ means $x$ directly influences $y$)
	- The overall probability distribution is factorized into component distributions held by the nodes
- In Bayesian Networks, the probability of a node $e_i$ only needs to be conditioned on its parents
$$Pr[e_{1},e_{2},...,e_{n}]=\prod_{i=1}^{n}Pr[e_{i}|Parents(e_{i})]$$
## Learning Bayesian Networks
- Learning Bayes Nets is computationally complex
- Learning a Bayesian Network consists of $2$ tasks: 
### Parameter Learning
- Parameter Learning includes learning the **Conditional Distribution Tables (CPTs)** and **Prior Probabilities** from the training data
- The goal is to maximize the joint probability given the training data
- It can be evaluated using the **Akaike Information Criterion (AIC)**
$$\text{AIC} = -2LL + 2K \text{ [cite: 677]}$$
- Where...
	- $K$ is the number of parameters in the network
	- $LL$ is the log-likelihood of the training data. 
- $\text{AIC}$ should be minimized
### Structure Learning
- Structure learning refers to figuring out the optimal connections between nodes by searching through the space of possible networks
- This is done by searching through edges (since nodes are fixed) using algorithms like $K2$ and  Tree Augmented Naive Bayes (TAN) 