# Basics
- Logistic Regression is used for **classification**
- For classification, using simple linear probability models does not work well, since they are not constrained to $[0, 1]$
# Logistic Regression Model
- The logistic regression (logit) model solves this by modelling the log odds of an event
$$\ln\left(\frac{p(X)}{1-p(X)}\right) = \beta_0 + \beta_1 X$$
- The logistic function is the inversion of the logit equation: 
$$\text{Pr}(X)=\frac{e^{\beta_0 + \beta_1 X }}{1 + e^{\beta_0 + \beta_1 X}}$$
- This equation constrains values to $[0, 1]$
![[logistic regression model.png|300]]
- In logistic regression...
	- ... $\beta_0$ is the regression constant
	- ... $\beta_1$ is the regression slope
	- ... $\frac{\beta_0}{\beta_1}$ is the threshold where the probability of success is 50%
## Odds and Logits
- Given an event $X$, and the probability $p(x)$, **odds** are defined as:
$$\text{odds}(X)= \frac{p(X)}{1-p(x)}$$
- If $\text{odds}(X) <1$, $p(X) < 0.5$
- **Logits** are calculated as: 
$$\ln(\text{odds}(X)) $$

# MLE
- To determine logistic regression coefficients, **Maximum Likelihood Estimation (MLE)** can be used 
- The probability of a single datapoint is modeled as a Bernoulli trial: 
$$p^{y+i}(1-p)^{1-y_i}$$
- The likelihood function $L$ for an iid sample is the joint probability of observing all data points
$$L: =\prod_{i=1}p^{y_i}(1-p)^{1-y_i} $$
- The term $p$ is replaced with the logistic function $\sigma(\beta_0 + \beta_1X)$ and the likelihood function becomes 
$$L = \prod_{=1}\sigma(\beta_0+\beta_1X_{1i})^{y_i}\cdot [1 - \sigma(\beta_0+\beta_{1i})]^{1-y_i}$$
- For simplicity, the **log-likelihood** is used
$$LL=\ln(L)=\sum_{i=1}y_i\ln p_i+(1-y_i)\ln(1-p_i)$$
- $\beta$ is chosen to maximize thelikelihood
- This can be doen through a numerical algorithm like gradient ascent
## Goodness of Fit
- To determine the goodness of fit of a model, two models are defined: 
	- The **null model**, where the only thing that matters is the intercept (overall probability)
	- The **fitted model**, which assumes that each data point can be explained with $p$ parameters and the intercept term
- The **null deviance** measures, how much is explained by the null model
$$-2\ln(L(null))$$
- The **residual deviance** measures how well the fitted model explains the data
$$-2\ln(L(fitted))$$
- Small values imply good fit
### Likelihood Ratio Test
- The **likelihood ratio test** is defined as: 
$$D=-2\ln\left(\frac{L(null)}{L(fitted)}\right) = -2(LL(null)-LL(fitted))$$
- $D$ follows a $\chi^2$ distribution
- Greater values indicate that the fitted model significantly improves the predictions compared to the null model
### Wald Test
- The Wald test is used to test the statistical significance of each individual coefficient in the model
- It tests the hypothesis $\beta_i= 0$
### McFadden $R^2$ 
- The McFadden $R^2$ tests goodness of fit by estimating: 
$$R^2_{MCFadden} = 1-\left(\frac{LL(fitted)}{LL(null)}\right)$$
- Values $>0.2$ are **acceptable and larger values are better**
## Coefficient interpretation
- The coefficient $\beta_1$ is directly related to the odds
- If $\beta_1 >0$, then $e^{\beta_1}>1$, meaning $X_1$ increases and the odds go up
- If $\beta_1 <0$, then $e^{\beta_1}<1$, meaning $X_1$ decreases and the odds go down
- $\beta_1$ is the **change in the log odds** for a **one unit change in $X$
- The odds ratio is defined as: 
$$OR=e^{\beta_1}$$
# Multiple Logistic Regression
- When given more than one independent variable, the equation expands to use all predictors
$$\ln\left(\frac{p(X)}{1-p(X)}\right) == \beta_0+\beta_1 X_1\beta_2 X_2+\dots + \beta_nX_n$$
- The coefficient $\beta_i$ represents the increase in log-odds for a one-unit increase in $X$
- The new probability equation is: 
$$\text{Pr}(X)=\frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1+\beta_2X_2+...\beta_nX_n)}}$$
# Multinomial Logit Models (#choices > 2)
- **Multinomial Logit Models** are used when the diescrete ordered variable $Y$ represents a choice or category from a set of more than two mutually excusive choices
# Generalized Linear Models (GLMs)
- Logistic regression is an example of a **Generalized Linear Model**
- GLMs are made up of 3 components: 
	1. A **distribution** for modeling 
	2. A **linear prediction** $\eta = \beta_0+\beta_1X_1 + ... + \beta_pX_p$  
	3. A **link function** $g(\cdot)$ linking the mean of the distribution ($\mu$) onto an unconstrained scale, allowing predictions to have a linear relation
## Count Variables
- Many dependent variables are counts (non-negative integers), such as number of crimes committed by a person
- Although such variables can be modeled with OLS, 2 limitations exist: 
	-  OLS could result in negative predictions
	- Oftentimes, count variables are highly skewed (most people commit no crimes, some commit many)
-  Common count models includ e **poisson regression** and **negative binomial regression models**
### Poisson
 - Count models often use the log-link function
 $$\ln(\mu) = \ln\left(e^{\beta'x}\right) = \beta'x$$
 - Coefficients are calculared through MLE
 - Poisson assumes that $\mathbb E(Y)= \text{Var}(Y) = \mu$
 -  If the variance is greater than the mean $\mu$, **overdispersion** often occurs
 - Overdispersion results in an underestimation of the standard error and overconfidence in the results
### Zero Inflation
- If a dataset has lots of zero vaues, this is called **zero-inflation**, leading it to be highly skewed
- Models tend to under-predict and they do not fit well 
- Zero inflated models assume two types of groups in a sample: 
	- **Type A:** are always zero and have no probability of a non-zero value
	- **Type ~A:** have a non-zero chance of a positive count value 
- Logit models can be used to determine group membership
- Poisson or negative binomial regression can be used for elements in $~A$
- The probabilities can be computed based on these results