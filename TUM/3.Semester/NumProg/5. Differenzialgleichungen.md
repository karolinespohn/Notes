- Fuer eine bessere Zusammenfassung empfehle ich die Notizen von [Anton](https://github.com/AntonScheitler/Notes)
# Grundlagen
- Differenzialgleichungen erlauben, eine Funktion ueber das Verhalten ihrer Ableitungen zu definieren
- Haeufig kann man Differenzialgleichugnen nicht analytisch, sondern nur numerisch loesen
- Man unterscheidet zwischen gewoehnlichen Differenzialgleichungen mit einer Variable und partiellen Differenzialgleichungen mit mehreren Variablen
- Sei $f$ eine Funktion und $\dot f$ ihre Ableitung
- Eine Differenzialgleichung in Abhaengigkeit von der Zeit $t$ laesst sich aufstellen wiefolgt:
$$\dot f(t) = F(t, f(t))$$
# Analytisches Verfahren
- Differenzialgleichungen koennen manchmal analytisch durch Separation der Variablen geloest werden
- Man schreibt sie dafuer in Leipniz-Notation, indem man $y(t)$ und $\dot y$ jeweils durch $y$ und $\frac{dy}{dt}$ ersetzt, und separiert die $t$s und $y$s
- Anschliessend bildet man auf der $y$-Seite das Integral von $y_0$ bis $y$ und auf der $t$-Seite das Integral von $t_0$ bis $t$
- Man erhaelt $y(t)$ schliesslich durch Umformen nach $y$
# Approximationsverfahren
- Um eine Differenzialgleichung $\dot f(t) = F(t, f(t))$ zu approxiomiert wird sie diskretisiert
- Jedes Vorkommen einer Ableitung wird durch den Differenzialquotienten ueber ein kleines Intervall $\delta t$ $\frac{y(t+\delta t)-y(t)}{\delta t}$ ersetzt
## Eulersche Methode
- Ein Startpunkt $y_0$ zum Zeitpunkt $t_0$ ist gegeben
- Um einen Wert $y_{k + 1}$ zum Zeitpunkt $t_{k + 1} = t_0 + (k + 1) \cdot \delta t$ zu ermitteln, werden der Differenzenquotient $f(t_k, y_k)$, sowie der Wert $y_k$ zum vorherigen Zeitpunkt $t_k$ verwendet
- Es gilt: 
$$y_{k + 1} = y_k + \delta t \cdot f(t_k, y_k)$$

![[Screenshot 2024-01-06 at 18.10.27.png|300]]
## Heun'sche Methode
- Mittels der euler'schen Methode bestimmt man die Werte $y_k$ und $y_{k+1}$ bestimmt
- Man ermittelt den Durchschnitt der Steigungen $f(t_k, y_k)$ und $f(t_{k + 1}, y_{k + 1})$ um die Steigung $t_k$ besser zu approximieren
- Hierbei verdoppelt sich die Anzahl der Auswertungen von $f$
- Man berechnet $y_{k+1}$ wiefolgt:
$$y_{k + 1} = y_k + \frac{\delta t}{2} \big (f(t_k, y_k) + f(t_{k+1}, y_k + \delta t \cdot f(t_k, y_k)) \big )$$
## Runge-Kutta Verfahren
- Zwischen den Zeitpunkten $t_k$ und $t_{k +1}$ definiert man zwei weitere Stuetzpunkte, sodass man insgesamt $4$ Punkte hat
- Man verwendet nun alle 4 Punkte fuer eine bessere Approximation fuer die Steigung zum Zeitpunkt $t_k$
- Die Anzahl der Auswertungen von $f$ vervierfacht sich
## Mehrschrittmethoden
- Man verwendet $f(t_k, y_k)$ und $f(t_{k-1}, y_{k-1})$ um $y_{k+1}$ zu ermitteln
- Man findet somit ein genaueres $y_{k + 1}$, ohne $f$ oefter auswerten zu muessen
- Man berechnet $y_{k+1}$ wiefolgt:
$$y_{k + 1} = y_k + \frac{\delta t}{2} \left (3f(t_k, y_k) - f(t_{k - 1}, y_{k - 1}) \right)$$
- Weil beim ersten Schritt kein Vorgaenger besteht, verwendet man fuer den einstieg eine Einschrittmethode
# Qualitaetsmessung
- Man bewertet ein Approximationsverfahren anhand der Konsistenz und Konvergenz
## Konsistenz
- Der lokale Fehler an einem Punkt $t_k$ bezeichnet den Fehler, der durch die Diskretisierung an einem Punkt entsteht
- Geht der lokale Fehler gegen 0 wenn $\delta t$ gegen 0 geht, ist ein Verfahren konsistent
## Konvergenz
- Der globale Fehler bezeichnet den maximalen Fehler der Summe aller Approximationen
- Geht der globale Fehler gegen 0, falls $\delta t$ gegen 0 geht, ist ein Verfahren konvergent
## Bewertung von Verfahren
- Euler, Heun und Runge-Kutta sind konsistent und konvergent

