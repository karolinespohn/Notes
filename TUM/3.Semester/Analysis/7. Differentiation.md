# Landau Symbole
- Seien $f$ und $g$ Funktionen mit Werten $\mathbb R ^n$ und sei $a$ ein Punkt (moeglicherweise auch $\pm \infty$)
- Falls $\forall x_n \to a, \space \limsup_{n \to \infty} \frac{||f(x)_n||}{||g(x_n)||} \lt \infty$
$$f \in \mathcal O(g)$$
- Falls $\forall x_n \to a, \space \lim_{n \to \infty} \frac{||f(x_n)||}{||g(x_n)||} = 0$
$$f \in \mathcal O(g)$$
- Falls $\forall x_n \to a,  f \in \mathcal O(g)$ und $g \in \mathcal O(f)$
$$f \in \Theta(g)$$

# innere Punkte
- Sei $D \subseteq \mathbb R$ 
- $x_0 \in D$ heisst ein innerer Punkt von $D$, falls $\exists \varepsilon >0$, so dass $(x_0-\varepsilon, x_0 + \varepsilon) \subseteq D$
## in hoeheren Dimensionen 
- Sei $D \subseteq \mathbb R^n$
- $x_0 \in D$ heisst ein innerer Punkt von $D$, falls $\exists \varepsilon >0$, so dass $\forall y \in \mathbb{R}$ gilt: $||y-x_0||< \varepsilon$ dann $y \in D$
# Ableitungen
- Sei $D \subseteq \mathbb{R}, \space x_0 \in D$ ein innerer Punkt und $f: D \to \mathbb R$ eine Funktion
- $a$ ist die Ableitung von $f$ in $x_0$ falls
$$a = \lim_{x \to x_0} \frac{f(x)-f(x_0)}{x-x_0} = f'(x_0)$$
- bzw falls:
$$f(x)-f(x_0) - a(x - x_0) \in o (x -x_0) \text{ in } x_0$$
- Das Differential ist die beste affine Approximation zu der Funktion $f$ in der Naehe vom Punkt $x_0$ 
- Die Newton'sche Schreibweise fuer die Ableitungen lautet $f'$, die Leibnitz'sche lautet $\frac{df}{dx}$ 
## Definition mittels Landau Symbolen
$$f'(x) = a \text{ falls } f(x)-f(x_0)-a(x-x_0) = \mathcal{o}(x-x_0) \text{ in } x_0$$
## Differenzierbarkeit
- $f$ heisst in einem Punkt $P$ differenzierbar, falls $f$ eine Ableitung in $P$ hat
## in hoeheren Dimensionen
- Sei $D \subseteq \mathbb R^n$, sei $x_0 \in D$ ein innerer Punkt, sei $f: D \to \mathbb R^m$ eine Funktion und sei $A \in Mat_{n \times m(\mathbb R)}$
- $A$ ist die Ableitung von f in $x_0$, falls 
- $f(x) -f(x_0) -A(x-x_0) = o(x-x_0)$
## Aus Differenzierbarkeit folgt Stetigkeit
- Falls $f$ in $x_0$ differenzierbar ist, dann ist $f$ in diesem Punkt stetig
- Es gilt nicht, $f$ differenzierbar ist, wenn $f$ stetig ist (beispielsweise $f(x) = |x|$)
## Differenzieren von $\exp$
- $\exp(x)$ ist differenzierbar:
$$\exp'(x) = \exp (x)$$
- Es sei $c$ eine Konstante. Falls $f' = f$, dann gilt:
$$f = c \cdot \exp$$
## Rechts- und linksseitige Ableitungen
 - Die linksseitige Ableitung der Funktion $f$ im Punkt $x_0$ ist:
$$\lim_{x \nearrow x_0} \frac{f(x) = f(x_0)}{x - x_0}$$
- Analog definiert man die rechtsseitige Ableitung
## Rechenregeln 
### Einfache arithmetische Operationen
- Falls $f$ und $g$ differenzierbar sind:
$$(f \pm g)' = f' \pm g'$$
$$(f \cdot g) = f \cdot g' + f' \cdot g$$
- Falls $f$ und $g$ differenzierbar sind, und $g \ne 0$
$$\left(\frac {f}{g}\right)' = \frac{f' \cdot g - f \cdot g'}{g^2}$$
### Kettenregel
- Seien $I, J \subseteq \mathbb R$, $f:I \to \mathbb R$, $f(I) \subseteq J$, $g: J \to \mathbb R$, $x_0$ ein innerer Punkt von $I$ und $f(x_0)$ ein innerer Punkt von $J$. 
- Ist $f$ differenzierbar in $x_0$ und $g$ differenzierbar in $f(x_0)$, so ist auch $g\circ f$ differenzierbar und es gilt:
$$(g \circ f )'(x)= g'(f(x_0))f'(x_0)$$
### Umkehrfunktionen
- Sei $f: D \to E$ eine Bijektion, $D, E \subseteq \mathbb R$, $x_0$ ein innerer Punkt von $D$, $f$ in $x_0$ differenzierbar und $f'(x_0) \neq 0$
- Sei weiters $y_0 = f(x_0)$ und $y_0$ ein innerer Punkt von $E$ 
- Dann ist $f^{-1}$ in $y_0$ differenzierbar und $(f^{-1}(y_0))' = \frac{1}{f'(x_0)}$ 
#### Beispiele
##### $\exp$ und $\ln$
 $$ln'(x) = \frac 1x$$
 ##### $\sin$ und $\arcsin$
 - Es gilt:
$$\sin'(x) = \cos(x)$$
$$\sin^2(x) + \cos^2(x)$$
- Daraus folgt:
$$\arcsin'(x) = \frac 1 {\sin'(\arcsin(x))} = \frac{1}{\cos(\arcsin(x))} = \frac{1}{\sqrt{1 - x^2}}$$
# Folgen und Reihen von Funktionen
## Punktweise Konvergenz
- Sei $D \subseteq \mathbb R (f_n)_n$ eine Folge von Funktionen $f_n: D \to \mathbb R$ 
- $(f_n)_n$ heisst Punktweise konvergent gegen $f$, falls gilt:
$$\forall x \in D: \lim_{n \to \infty}f_n(x) = f(x)$$
- Die Reihe $\sum f_n$ konvergiert Punktweise gegen $f$, falls gilt:
$$\forall x \in D: \sum^{\infty}_{n = 1}f_n(x) =f(x)$$
## Gleichmaessige Konvergenz
- $(f_n)_n$ heisst gleichmaessig konvergent gegen $f$, falls: $\forall \varepsilon > 0 \space \exists  N$, sodass gilt: 
$$\forall n >N, \forall x \in D: |f_n(x) - f(x)| < \varepsilon$$
- Gleichmaessige Konvergenz impliziert punktweise Konvergenz
### Saetze
- Falls alle $(f_n)$ stetig sind und $f_n \to f$ gleichmaessig ist, dann ist $f$ stetig
- Sei $(f_n)$ eine Folge differenzierbarer Funktionen und nimm an, dass die Folge $(f'_n)$ gleichmaessig gegen eine Funktion $g$ konvergiert
- Nimm an, dass $(f_n)$ punktweise gegen eine Funktion $f$ konvergiert
- Dann ist $f$ auch differenzierbar und $f' = g$
- Alles funktioniert auch fuer Reihen
## Extrema einer Funktion
### globale Maxima
- Sei $f: D \to \mathbb R$ eine Funktion
- Man nennt $x_0$ globales Maximum von $f$ falls gilt:
$$\forall x \in D: f(x) \le f(x_0)$$
- Analog definiert man globale Maxima
### lokale Maxima
- Es sei $x_0$ ein innerer Punkt und $\exists \space \varepsilon \gt 0$, so dass $(x_0-\varepsilon, x_0 + \varepsilon) \subseteq D$ 
- Weiters sei $x_0$ ein globales Maximum fuer $f((x_0-\varepsilon, x_0 + \varepsilon))$ 
- Dann nennt man $x_0$ lokales Maximum
- Analog definiert lokale Minima
### globale und lokale Extrema
- $x_0$ heisst globales Extremum, falls $x_0$ ein globales Minimum oder Minimum ist
- $x_0$ heisst lokales Extremum, falls $x_0$ ein lokales Maximum oder Minimum ist
### Lemma: Ableitung an Extrema 
- Sei $f: D \to \mathbb R$ eine Funktion mit dem inneren Punkt $x_0 \in D$
- Sei weiters $x_0$ ein lokales Extremum fuer $f$ und $f$ in $x_0$ differenzierbar
- Dann gilt:
$$f'(x_0) = 0$$
## Satz von Rolle
- Sei $f:[a, b] \to \mathbb R$ eine Funktion, die in $[a, b]$ stetig ist, in $(a, b)$ differenzierbar ist und $f(a) = f(b)$
- Dann $\exists \space x_0 \in (a, b)$, sodass $f(x_0) = 0$
## Mittelwertsatz
- Sei $f:[a, b] \to \mathbb R$ eine Funktion, die auf $[a, b]$ stetig ist und auf $(a, b)$ differenzierbar ist
- Dann existsiert $x_0 \in (a, b)$, sodass:
$$f'(x_0)=\frac{f(b)-f(a)}{b -a}$$
- Es gibt einen Punkt, in dem die Steigung gleich der Durchschnittlichen Steigung in diesem Intervall ist
## Ableitungen monotoner Funktionen
- Sei $f:(a, b) \in \mathbb R$ differenzierbar
- Falls $f'(x)> 0 \space \forall \space x \in (a, b)$, dann ist $f$ streng monoton wachsend
- Falls $f'(x)< 0\forall \forall \space x \in (a, b)$, dann ist $f$ streng monoton fallend
- Falls $f'(x)\ge 0 \space \forall \space x \in (a, b)$, dann ist $f$ monoton wachsend
- Falls $f'(x)\le 0 \space \forall \space x \in (a, b)$, dann ist $f$ monoton fallend
## Kriterien fuer Extrema
- Sei $f: (a, b)$ differenzierbar und $x_0 \in (a, b)$ mit $f'(x_0) = 0$
- Falls $f' \ge 0$ in $(a, x_0)$, und $f'\le 0$ in $(x_0, b)$, ist $x_0$ ein Maximum
- Ist $f$ zweimal in $x_0$ differenzierbar, und $f'(x_0) = 0$ und $f''(x_0) < 0$, dann ist $x_0$ ein Maximum
- Falls $f' \le 0$ in $(a, x_0)$, und $f'\ge 0$ in $(x_0, b)$, ist $x_0$ ein Minimum
- Ist $f$ zweimal in $x_0$ differenzierbar, und $f'(x_0) = 0$ und $f''(x_0) > 0$, dann ist $x_0$ ein Minimum
## Satz von L'Hopital 
- Seien $f, g$ in $(a, b)$ differenzierbar und es gelte:
$$\lim_{x \to b}f (x) = \lim_{x \to b}g(x) = 0$$
$$\forall x \in (a, b): g(x) \neq 0$$
- Falls $\lim_{x \to b} \frac{f'(x)}{g'(x)}$ existiert, dann gilt:
$$\lim_{x \to b} = \frac {f(x)}{g(x)} = \lim_{x \to b} = \frac {f'(x)}{g'(x)} $$
# Konvexitaet und die 2. Ableitung
- Sei $I$ ein Intervall und Seit $f: I \to \mathbb R$ eine Funktion
- Man nennt $f$ konvex, wenn gilt: 
$$\forall \space x, y, \in I, \forall \space \gamma \in [0, 1]: f(\gamma x + (1 -\gamma)y) \le \gamma f (x)+(1-\gamma)f(y)$$
- $f$ heisst streng konvex, falls $x \neq y$ und $\gamma \in (0, 1)$
- $f$ heisst (streng) konkav, falls $-f$ (streng) konvex ist
## 2. Ableitung
- $f$ heisst zweimal differenzierbar, falls $f$ differenzierbar ist, und $f'$ differenzierbar ist
- Wir schreiben $f''$ fuer die Ableitung von $f'$
## Satz: Konvexitaet mittels der Steigung der Ableitung
- Sei $f: I \to \mathbb R$ eine differenzierbare Funktion
- falls $f'$ monoton wachsend ist, ist f konvex
- Ist also $f$ zweimal differenzierbar und $f''$ nicht negativ, so ist $f$ konvex
- Ist $f$ zweimal differenzierbar und $f''$ nicht positiv, so ist $f$ konkav
## Ungleichung von Jensen
- Seien $\alpha_1, \alpha_2, ..., \alpha_n > 0$ so dass $\alpha_1 + \alpha_2+...+\alpha_n = 1$
- Sei $f$ konvex und seien $x_1, ...x_n$ im Definitionsvereich von $f$
- Dann gilt:
$$\sum^{n}_{k = 1}\alpha_nf(x_n) \ge f \left(\sum^{n}_{k =1}\alpha_nx_n\right)$$
- Ist $f$ konkav gilt:
$$\sum^{n}_{k = 1}\alpha_nf(x_n) \le f \left(\sum^{n}_{k =1}\alpha_nx_n\right)$$
